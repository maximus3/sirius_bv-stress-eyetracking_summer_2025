import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import (
    ttest_ind,
    mannwhitneyu,
)
import os
import warnings
import argparse

# =============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò –ö–û–ù–°–¢–ê–ù–¢–´
# =============================================================================


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
def parse_arguments():
    """–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(
        description="–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∞–π—Ç—Ä–µ–∫–∏–Ω–≥–∞ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å—Ç—Ä–µ—Å—Å–∞",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument(
        "--word-file",
        type=str,
        default="data/ia_avg.xls",
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤",
    )

    parser.add_argument(
        "--trial-file",
        type=str,
        default="data/events.xls",
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–π–ª–æ–≤",
    )

    parser.add_argument(
        "--results-dir",
        type=str,
        default="results",
        help="–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
    )

    parser.add_argument(
        "--show-plots",
        action="store_true",
        help="–û—Ç–æ–±—Ä–∞–∂–∞—Ç—å —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ç–æ–ª—å–∫–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å)",
    )

    return parser.parse_args()


# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–±—É–¥—É—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤)
WORD_DATA_FILE = None
TRIAL_DATA_FILE = None
RESULTS_DIR = None
SHOW_PLOTS = False

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞–Ω–Ω—ã—Ö
WORD_NUMERIC_COLUMNS_POSITIONS = [3, 4, 7, 8, 9, 10, 11]
WORD_COLUMN_FOR_FILTERING = 6  # –ö–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏

# –ü–æ—Ä–æ–≥–∏ –¥–ª—è –≥—Ä—É–ø–ø —Å—Ç—Ä–µ—Å—Å–∞
STRESS_THRESHOLD = 3  # –¢—Ä–∞–π–ª—ã 1-STRESS_THRESHOLD –±–µ–∑ —Å—Ç—Ä–µ—Å—Å–∞, –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–æ —Å—Ç—Ä–µ—Å—Å–æ–º

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–∞–π–ª–æ–≤ (–±—É–¥—É—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö)
TOTAL_TRIALS = None  # –ë—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö
MAX_TRIAL_NUMBER = None  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –Ω–æ–º–µ—Ä —Ç—Ä–∞–π–ª–∞

# –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
ALPHA_LEVEL = 0.05
MIN_SAMPLE_SIZE_WARNING = 30  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –Ω–∞–¥–µ–∂–Ω—ã—Ö –≤—ã–≤–æ–¥–æ–≤

# –ü–æ—Ä–æ–≥–∏ —Ä–∞–∑–º–µ—Ä–æ–≤ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ Cohen's d
EFFECT_SIZE_SMALL = 0.2
EFFECT_SIZE_MEDIUM = 0.5
EFFECT_SIZE_LARGE = 0.8

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
FIGURE_DPI = 300
COLORS_NO_STRESS = "#2E86C1"  # –°–∏–Ω–∏–π
COLORS_STRESS = "#E74C3C"  # –ö—Ä–∞—Å–Ω—ã–π

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
SHOW_STATISTICAL_WARNINGS = False

# =============================================================================

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ warnings
if not SHOW_STATISTICAL_WARNINGS:
    warnings.filterwarnings("ignore")
    # –ü–æ–¥–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ matplotlib warnings
    warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")
    warnings.filterwarnings("ignore", message=".*labels.*parameter.*boxplot.*")
    warnings.filterwarnings("ignore", message=".*Glyph.*missing from font.*")
    plt.set_loglevel("WARNING")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –∫—Ä–∞—Å–∏–≤—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤
plt.style.use("seaborn-v0_8")
plt.rcParams["font.family"] = ["DejaVu Sans"]
plt.rcParams["font.size"] = 12
plt.rcParams["axes.titlesize"] = 14
plt.rcParams["axes.labelsize"] = 12


def ensure_results_directory():
    """–°–æ–∑–¥–∞–µ—Ç –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
    if not os.path.exists(RESULTS_DIR):
        os.makedirs(RESULTS_DIR)
        print(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ '{RESULTS_DIR}' –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤")
    else:
        print(f"üìÅ –ü–∞–ø–∫–∞ '{RESULTS_DIR}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")


def show_plot_conditionally():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ñ–ª–∞–≥ SHOW_PLOTS"""
    if SHOW_PLOTS:
        plt.show()
    else:
        plt.close()  # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∏–≥—É—Ä—É –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏


def determine_trial_parameters(trial_data):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–∞–π–ª–æ–≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
    global TOTAL_TRIALS, MAX_TRIAL_NUMBER

    TOTAL_TRIALS = len(trial_data)
    MAX_TRIAL_NUMBER = trial_data["trial"].max()

    print(f"üìä –û–ü–†–ï–î–ï–õ–ï–ù–´ –ü–ê–†–ê–ú–ï–¢–†–´ –¢–†–ê–ô–õ–û–í:")
    print(f"   ‚Ä¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–π–ª–æ–≤: {TOTAL_TRIALS}")
    print(f"   ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –Ω–æ–º–µ—Ä —Ç—Ä–∞–π–ª–∞: {MAX_TRIAL_NUMBER}")
    print(
        f"   ‚Ä¢ –ü–æ—Ä–æ–≥ —Å—Ç—Ä–µ—Å—Å–∞: —Ç—Ä–∞–π–ª—ã 1-{STRESS_THRESHOLD} (–±–µ–∑ —Å—Ç—Ä–µ—Å—Å–∞), {STRESS_THRESHOLD + 1}-{MAX_TRIAL_NUMBER} (—Å—Ç—Ä–µ—Å—Å)"
    )

    return TOTAL_TRIALS, MAX_TRIAL_NUMBER


def create_dynamic_phase_mapping(max_trial):
    """–°–æ–∑–¥–∞–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –º–∞–ø–ø–∏–Ω–≥ —Ñ–∞–∑ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç—Ä–∞–π–ª–æ–≤"""
    phase_mapping = {}

    # –ë–∞–∑–æ–≤–∞—è –ª–∏–Ω–∏—è: —Ç—Ä–∞–π–ª—ã 1 –¥–æ STRESS_THRESHOLD
    for i in range(1, STRESS_THRESHOLD + 1):
        if i <= max_trial:
            phase_mapping[i] = f"baseline_{i}"

    # –°—Ç—Ä–µ—Å—Å–æ–≤—ã–µ —Ç—Ä–∞–π–ª—ã: –æ—Ç STRESS_THRESHOLD+1 –¥–æ max_trial
    stress_trials = list(range(STRESS_THRESHOLD + 1, max_trial + 1))

    if len(stress_trials) == 0:
        # –ù–µ—Ç —Å—Ç—Ä–µ—Å—Å–æ–≤—ã—Ö —Ç—Ä–∞–π–ª–æ–≤
        pass
    elif len(stress_trials) == 1:
        # –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω —Å—Ç—Ä–µ—Å—Å–æ–≤—ã–π —Ç—Ä–∞–π–ª
        phase_mapping[stress_trials[0]] = "stress_peak"
    elif len(stress_trials) == 2:
        # –î–≤–∞ —Å—Ç—Ä–µ—Å—Å–æ–≤—ã—Ö —Ç—Ä–∞–π–ª–∞: –ø–∏–∫ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
        phase_mapping[stress_trials[0]] = "stress_peak"
        phase_mapping[stress_trials[1]] = "stress_recovery"
    else:
        # –¢—Ä–∏ –∏ –±–æ–ª–µ–µ —Å—Ç—Ä–µ—Å—Å–æ–≤—ã—Ö —Ç—Ä–∞–π–ª–æ–≤: –ø–∏–∫, –∞–¥–∞–ø—Ç–∞—Ü–∏—è(–∏), –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
        phase_mapping[stress_trials[0]] = "stress_peak"
        phase_mapping[stress_trials[-1]] = "stress_recovery"

        # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ç—Ä–∞–π–ª—ã - –∞–¥–∞–ø—Ç–∞—Ü–∏—è
        for i, trial_num in enumerate(stress_trials[1:-1], 1):
            phase_mapping[trial_num] = f"stress_adapt_{i}"

    return phase_mapping


def validate_sample_size(n, analysis_name="–∞–Ω–∞–ª–∏–∑"):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏ –∏ –≤—ã–≤–æ–¥–∏—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è"""
    if n < MIN_SAMPLE_SIZE_WARNING:
        print(f"üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï ({analysis_name}):")
        print(f"   –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ N={n} –ö–†–ò–¢–ò–ß–ï–°–ö–ò –ú–ê–õ!")
        print(
            f"   –î–ª—è –Ω–∞–¥–µ–∂–Ω—ã—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã–≤–æ–¥–æ–≤ —Ç—Ä–µ–±—É–µ—Ç—Å—è N ‚â• {MIN_SAMPLE_SIZE_WARNING}"
        )
        print(f"   ‚ö†Ô∏è –í–°–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–û–ì–£–¢ –ë–´–¢–¨ –ù–ï–ù–ê–î–ï–ñ–ù–´–ú–ò!")
        return False
    return True


def apply_bonferroni_correction(p_values, alpha=ALPHA_LEVEL):
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø–æ–ø—Ä–∞–≤–∫—É –ë–æ–Ω—Ñ–µ—Ä—Ä–æ–Ω–∏ –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    if not p_values or len(p_values) == 0:
        return []

    corrected_alpha = alpha / len(p_values)
    corrected_p_values = [min(1.0, p * len(p_values)) for p in p_values]

    print(f"üîß –ü–û–ü–†–ê–í–ö–ê –ë–û–ù–§–ï–†–†–û–ù–ò:")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤: {len(p_values)}")
    print(f"   –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Œ±: {corrected_alpha:.4f}")

    return corrected_p_values, corrected_alpha


def get_phase_color(phase_name):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ü–≤–µ—Ç –¥–ª—è –ª—é–±–æ–π —Ñ–∞–∑—ã, –≤–∫–ª—é—á–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ"""
    if phase_name.startswith("baseline"):
        baseline_colors = ["#3498DB", "#5DADE2", "#85C1E9"]
        try:
            baseline_num = int(phase_name.split("_")[1]) - 1
            return (
                baseline_colors[baseline_num]
                if baseline_num < len(baseline_colors)
                else "#85C1E9"
            )
        except:
            return "#3498DB"
    elif phase_name == "stress_peak":
        return COLORS_STRESS
    elif phase_name.startswith("stress_adapt"):
        return "#F1948A"
    elif phase_name == "stress_recovery":
        return "#F8C471"
    else:
        return "#95A5A6"  # –°–µ—Ä—ã–π –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ñ–∞–∑


def interpret_effect_size_with_warnings(cohens_d, p_value, n_total, variable_name):
    """
    –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞ —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏ –æ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
    """
    abs_d = abs(cohens_d)

    if abs_d >= EFFECT_SIZE_LARGE:
        size_text = "–±–æ–ª—å—à–æ–π"
    elif abs_d >= EFFECT_SIZE_MEDIUM:
        size_text = "—Å—Ä–µ–¥–Ω–∏–π"
    elif abs_d >= EFFECT_SIZE_SMALL:
        size_text = "–º–∞–ª—ã–π"
    else:
        size_text = "–ø—Ä–µ–Ω–µ–±—Ä–µ–∂–∏–º–æ –º–∞–ª—ã–π"

    # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
    warnings_list = []

    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –º–∞–ª–æ–π –≤—ã–±–æ—Ä–∫–µ
    if n_total < MIN_SAMPLE_SIZE_WARNING:
        warnings_list.append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–∞–ª—ã–π —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ (N={n_total})")

    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –Ω–µ–∑–Ω–∞—á–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
    if p_value >= ALPHA_LEVEL:
        warnings_list.append(
            "–†–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞ –Ω–µ–Ω–∞–¥–µ–∂–µ–Ω –ø—Ä–∏ –Ω–µ–∑–Ω–∞—á–∏–º–æ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ"
        )
        if n_total < 20:
            warnings_list.append(
                "–ü—Ä–∏ –º–∞–ª–æ–π –≤—ã–±–æ—Ä–∫–µ –∏ p‚â•0.05 Cohen's d –º–æ–∂–µ—Ç –≤–≤–æ–¥–∏—Ç—å –≤ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–µ"
            )

    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ–± —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö
    if abs_d > 2.0 and p_value >= ALPHA_LEVEL:
        warnings_list.append(
            "–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞ –ø—Ä–∏ –Ω–µ–∑–Ω–∞—á–∏–º–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ"
        )

    result_text = f"üìè –†–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞: {size_text} (d = {cohens_d:.3f})"

    if warnings_list:
        result_text += "\n   ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø –û –ù–ê–î–ï–ñ–ù–û–°–¢–ò:"
        for warning in warnings_list:
            result_text += f"\n     - {warning}"

        # –û—Å–æ–±–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
        if p_value >= ALPHA_LEVEL and n_total < MIN_SAMPLE_SIZE_WARNING:
            result_text += f"\n   üö® –ù–ï –ò–ù–¢–ï–†–ü–†–ï–¢–ò–†–û–í–ê–¢–¨ –∫–∞–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ —Ä–∞–∑–ª–∏—á–∏–π!"

    return result_text, warnings_list


def load_comprehensive_data():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∞–π—Ç—Ä–µ–∫–∏–Ω–≥–∞ –∏–∑ –¥–≤—É—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:
    1. ia_avg.xls - –¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
    2. events.xls - –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–π–ª–æ–≤
    """
    print("üîÑ –ö–û–ú–ü–õ–ï–ö–°–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ê–ô–¢–†–ï–ö–ò–ù–ì–ê")
    print("=" * 70)

    # 1. –î–ê–ù–ù–´–ï –ù–ê –£–†–û–í–ù–ï –°–õ–û–í
    print("üìñ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤...")
    word_data = pd.read_csv(WORD_DATA_FILE, sep="\t", encoding="utf-16", skiprows=1)
    col_names = list(word_data.columns)

    # –°–æ–∑–¥–∞–µ–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É –ø–æ —É—Å–ª–æ–≤–∏—è–º –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
    trial_col = col_names[0]
    word_data["trial"] = word_data[trial_col].astype(int)
    word_data["condition"] = word_data["trial"].apply(
        lambda x: "no_stress" if x <= STRESS_THRESHOLD else "stress"
    )

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –Ω–æ–º–µ—Ä —Ç—Ä–∞–π–ª–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –º–∞–ø–ø–∏–Ω–≥–∞ —Ñ–∞–∑
    max_trial_in_words = word_data["trial"].max()
    phase_mapping = create_dynamic_phase_mapping(max_trial_in_words)

    # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É –ø–æ —Ñ–∞–∑–∞–º –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
    word_data["stress_phase"] = word_data["trial"].map(phase_mapping)

    # –†—É—Å—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —Å–ª–æ–≤
    word_measure_names = {
        "IA_FIRST_FIXATION_DURATION": "–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–µ—Ä–≤–æ–π —Ñ–∏–∫—Å–∞—Ü–∏–∏ (–º—Å)",
        "IA_FIXATION_COUNT": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–∫—Å–∞—Ü–∏–π –Ω–∞ —Å–ª–æ–≤–æ",
        "IA_DWELL_TIME": "–û–±—â–µ–µ –≤—Ä–µ–º—è –ø—Ä–µ–±—ã–≤–∞–Ω–∏—è (–º—Å)",
        "IA_DWELL_TIME_%": "–ü—Ä–æ—Ü–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ –ø—Ä–µ–±—ã–≤–∞–Ω–∏—è (%)",
        "IA_VISITED_TRIAL_%": "–ü—Ä–æ—Ü–µ–Ω—Ç –≤–∏–∑–∏—Ç–æ–≤ –∫ —Å–ª–æ–≤—É (%)",
        "IA_REVISIT_TRIAL_%": "–ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤–∏–∑–∏—Ç–æ–≤ (%)",
        "IA_RUN_COUNT": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–±–µ–≥–∞–Ω–∏–π –≤–∑–≥–ª—è–¥–∞",
    }

    # –ù–∞—Ö–æ–¥–∏–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å–ª–æ–≤
    word_numeric_cols = [
        col_names[i] for i in WORD_NUMERIC_COLUMNS_POSITIONS if i < len(col_names)
    ]
    word_short_names = [
        "IA_FIRST_FIXATION_DURATION",
        "IA_FIXATION_COUNT",
        "IA_DWELL_TIME_%",
        "IA_DWELL_TIME",
        "IA_VISITED_TRIAL_%",
        "IA_REVISIT_TRIAL_%",
        "IA_RUN_COUNT",
    ]

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
    for col in word_numeric_cols:
        if col in word_data.columns:
            word_data[col] = pd.to_numeric(
                word_data[col].astype(str).str.replace(",", "."), errors="coerce"
            )

    # –°–æ–∑–¥–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è —Å–ª–æ–≤
    for i, col in enumerate(word_numeric_cols):
        if col in word_data.columns and i < len(word_short_names):
            word_data[word_short_names[i]] = word_data[col]

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
    word_col = col_names[WORD_COLUMN_FOR_FILTERING]
    word_data = word_data.dropna(subset=[word_col])
    word_data = word_data[word_data[word_col] != "."]
    word_data = word_data[word_data[word_col] != "‚Äì"]
    word_data = word_data[word_data[word_col] != "‚Äî"]

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(word_data)} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤")

    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —Å–ª–æ–≤
    validate_sample_size(len(word_data), "–¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤")

    # 2. –î–ê–ù–ù–´–ï –ù–ê –£–†–û–í–ù–ï –¢–†–ê–ô–õ–û–í
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–π–ª–æ–≤...")
    trial_data = pd.read_csv(TRIAL_DATA_FILE, sep="\t", encoding="utf-16", header=0)

    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫—É —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –∏ –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ
    trial_data = trial_data.iloc[1:].reset_index(drop=True)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∑–∞–ø—è—Ç—ã–µ –≤ —Ç–æ—á–∫–∏ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∏—Å–ª–∞
    for col in trial_data.columns:
        trial_data[col] = pd.to_numeric(
            trial_data[col].astype(str).str.replace(",", "."), errors="coerce"
        )

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–∞–π–ª–∞—Ö
    trial_data["trial"] = range(1, len(trial_data) + 1)
    trial_data["condition"] = trial_data["trial"].apply(
        lambda x: "no_stress" if x <= STRESS_THRESHOLD else "stress"
    )

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–∞–π–ª–æ–≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏
    determine_trial_parameters(trial_data)

    # –°–æ–∑–¥–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –º–∞–ø–ø–∏–Ω–≥ —Ñ–∞–∑
    phase_mapping = create_dynamic_phase_mapping(MAX_TRIAL_NUMBER)
    trial_data["phase"] = trial_data["trial"].map(phase_mapping)

    # –í—ã—á–∏—Å–ª—è–µ–º –¥–æ–ª—é –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö –∑–æ–Ω (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö) –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–º
    # INTEREST_AREA_COUNT - –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω—è—è –∫–æ–ª–æ–Ω–∫–∞ (–∏–Ω–¥–µ–∫—Å -2)
    # VISITED_INTEREST_AREA_COUNT - –ø–æ—Å–ª–µ–¥–Ω—è—è –∫–æ–ª–æ–Ω–∫–∞ (–∏–Ω–¥–µ–∫—Å -1)
    interest_areas_count = pd.to_numeric(
        trial_data.iloc[:, 13], errors="coerce"
    )  # INTEREST_AREA_COUNT
    visited_areas_count = pd.to_numeric(
        trial_data.iloc[:, 14], errors="coerce"
    )  # VISITED_INTEREST_AREA_COUNT

    # –°–æ–∑–¥–∞–µ–º –¥–æ–ª—é –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö –∑–æ–Ω –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
    visited_areas_percent = (visited_areas_count / interest_areas_count * 100).fillna(0)

    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    trial_data.columns = [
        "blinks",
        "fixations",
        "fixation_duration_mean",
        "fixation_duration_median",
        "fixation_duration_sd",
        "pupil_size",
        "runs",
        "saccade_amplitude_mean",
        "saccade_amplitude_median",
        "saccade_amplitude_sd",
        "saccades",
        "samples",
        "trial_duration",
        "interest_areas_total",
        "visited_areas_absolute",
        "trial",
        "condition",
        "phase",
    ]

    # –ó–∞–º–µ–Ω—è–µ–º –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞ –¥–æ–ª—é –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
    trial_data["visited_areas"] = visited_areas_percent

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(trial_data)} —Ç—Ä–∞–π–ª–æ–≤")
    print(
        f"   ‚Ä¢ –ë–µ–∑ —Å—Ç—Ä–µ—Å—Å–∞ (—Ç—Ä–∞–π–ª—ã 1-{STRESS_THRESHOLD}): {len(trial_data[trial_data['condition'] == 'no_stress'])} —Ç—Ä–∞–π–ª–∞"
    )
    print(
        f"   ‚Ä¢ –°–æ —Å—Ç—Ä–µ—Å—Å–æ–º (—Ç—Ä–∞–π–ª—ã {STRESS_THRESHOLD + 1}-{MAX_TRIAL_NUMBER}): {len(trial_data[trial_data['condition'] == 'stress'])} —Ç—Ä–∞–π–ª–∞"
    )

    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —Ç—Ä–∞–π–ª–æ–≤
    validate_sample_size(len(trial_data), "–¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–π–ª–æ–≤")

    return word_data, trial_data, word_measure_names


def analyze_word_level_differences(word_data, word_measure_names):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –º–µ–∂–¥—É —É—Å–ª–æ–≤–∏—è–º–∏ –±–µ–∑ —Å—Ç—Ä–µ—Å—Å–∞ –∏ —Å–æ —Å—Ç—Ä–µ—Å—Å–æ–º
    """
    print(f"\nüìù –ê–ù–ê–õ–ò–ó –†–ê–ó–õ–ò–ß–ò–ô –ù–ê –£–†–û–í–ù–ï –°–õ–û–í")
    print("=" * 70)

    measures = [
        "IA_FIRST_FIXATION_DURATION",
        "IA_FIXATION_COUNT",
        "IA_DWELL_TIME",
        "IA_DWELL_TIME_%",
        "IA_VISITED_TRIAL_%",
        "IA_REVISIT_TRIAL_%",
        "IA_RUN_COUNT",
    ]

    results = []
    p_values_for_correction = []

    # –î–∞–Ω–Ω—ã–µ –ø–æ —É—Å–ª–æ–≤–∏—è–º
    no_stress = word_data[word_data["condition"] == "no_stress"]
    stress = word_data[word_data["condition"] == "stress"]

    print(f"üìä –°–†–ê–í–ù–ï–ù–ò–ï –ì–†–£–ü–ü –ù–ê –£–†–û–í–ù–ï –°–õ–û–í:")
    print(f"   –ë–µ–∑ —Å—Ç—Ä–µ—Å—Å–∞: {len(no_stress)} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π")
    print(f"   –°–æ —Å—Ç—Ä–µ—Å—Å–æ–º: {len(stress)} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏
    total_n = len(no_stress) + len(stress)
    validate_sample_size(total_n, "–∞–Ω–∞–ª–∏–∑ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤")

    for measure in measures:
        if measure not in word_data.columns:
            continue

        print(f"\nüìà {word_measure_names.get(measure, measure)}")
        print("-" * 50)

        no_stress_vals = no_stress[measure].dropna()
        stress_vals = stress[measure].dropna()

        if len(no_stress_vals) == 0 or len(stress_vals) == 0:
            continue

        # –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        ns_mean, ns_std = no_stress_vals.mean(), no_stress_vals.std()
        s_mean, s_std = stress_vals.mean(), stress_vals.std()

        print(f"–ë–µ–∑ —Å—Ç—Ä–µ—Å—Å–∞: M = {ns_mean:.2f} ¬± {ns_std:.2f}")
        print(f"–°–æ —Å—Ç—Ä–µ—Å—Å–æ–º:  M = {s_mean:.2f} ¬± {s_std:.2f}")

        # –ò–∑–º–µ–Ω–µ–Ω–∏–µ
        change = s_mean - ns_mean
        change_pct = (change / ns_mean) * 100 if ns_mean != 0 else 0
        change_symbol = "üìà" if change > 0 else "üìâ"
        change_direction = "—É–≤–µ–ª–∏—á–µ–Ω–∏–µ" if change > 0 else "—É–º–µ–Ω—å—à–µ–Ω–∏–µ"
        print(f"{change_symbol} –ò–∑–º–µ–Ω–µ–Ω–∏–µ: {change:+.2f} ({change_pct:+.1f}%)")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç (–ø—Ä–æ–≤–µ—Ä–∫–∞ H0: Œº‚ÇÅ = Œº‚ÇÇ –ø—Ä–æ—Ç–∏–≤ H1: Œº‚ÇÅ ‚â† Œº‚ÇÇ)
        try:
            stat, p_value = mannwhitneyu(
                no_stress_vals, stress_vals, alternative="two-sided"
            )
            test_name = "–ö—Ä–∏—Ç–µ—Ä–∏–π –ú–∞–Ω–Ω–∞-–£–∏—Ç–Ω–∏"
        except:
            try:
                stat, p_value = ttest_ind(no_stress_vals, stress_vals)
                test_name = "t-–∫—Ä–∏—Ç–µ—Ä–∏–π"
            except:
                p_value = np.nan
                test_name = "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö"

        if not np.isnan(p_value):
            if p_value < ALPHA_LEVEL:
                significance = "‚úÖ H0 –û–¢–ö–õ–û–ù–Ø–ï–¢–°–Ø (p < 0.05)"
                hypothesis_result = "–†–∞–∑–ª–∏—á–∏—è –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò –ó–ù–ê–ß–ò–ú–´"
            else:
                significance = "üî∏ H0 –ù–ï –û–¢–ö–õ–û–ù–Ø–ï–¢–°–Ø (p ‚â• 0.05)"
                hypothesis_result = "–†–∞–∑–ª–∏—á–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –ù–ï –ó–ù–ê–ß–ò–ú–´"
            print(f"üß™ {test_name}: p = {p_value:.4f} - {significance}")
            print(f"‚öñÔ∏è –ó–∞–∫–ª—é—á–µ–Ω–∏–µ: {hypothesis_result}")
            p_values_for_correction.append(p_value)
        else:
            print(f"üß™ {test_name}: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥–∏–ø–æ—Ç–µ–∑")
            p_values_for_correction.append(np.nan)

        # –†–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞ (Cohen's d) —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏
        pooled_std = np.sqrt(
            ((len(no_stress_vals) - 1) * ns_std**2 + (len(stress_vals) - 1) * s_std**2)
            / (len(no_stress_vals) + len(stress_vals) - 2)
        )
        cohens_d = (s_mean - ns_mean) / pooled_std if pooled_std > 0 else 0

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏
        effect_interpretation, warnings = interpret_effect_size_with_warnings(
            cohens_d, p_value, len(no_stress_vals) + len(stress_vals), measure
        )
        print(effect_interpretation)

        results.append(
            {
                "measure": measure,
                "measure_name": word_measure_names.get(measure, measure),
                "mean_no_stress": ns_mean,
                "std_no_stress": ns_std,
                "mean_stress": s_mean,
                "std_stress": s_std,
                "change_absolute": change,
                "change_percent": change_pct,
                "change_direction": change_direction,
                "p_value": p_value,
                "cohens_d": cohens_d,
                "significant": p_value < ALPHA_LEVEL
                if not np.isnan(p_value)
                else False,
                "reliability_warnings": warnings,
            }
        )

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ–ø—Ä–∞–≤–∫—É –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    if len(p_values_for_correction) > 1:
        print(f"\nüîß –ü–û–ü–†–ê–í–ö–ê –ù–ê –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–´–ï –°–†–ê–í–ù–ï–ù–ò–Ø:")
        valid_p_values = [p for p in p_values_for_correction if not np.isnan(p)]
        if valid_p_values:
            corrected_p_values, corrected_alpha = apply_bonferroni_correction(
                valid_p_values
            )
            significant_after_correction = sum(
                1 for p in corrected_p_values if p < ALPHA_LEVEL
            )
            print(
                f"   –ü–æ—Å–ª–µ –ø–æ–ø—Ä–∞–≤–∫–∏ –ë–æ–Ω—Ñ–µ—Ä—Ä–æ–Ω–∏: {significant_after_correction}/{len(valid_p_values)} –∑–Ω–∞—á–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
            )

    return results


def analyze_trial_level_differences(trial_data):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–π–ª–æ–≤ –º–µ–∂–¥—É —É—Å–ª–æ–≤–∏—è–º–∏ –±–µ–∑ —Å—Ç—Ä–µ—Å—Å–∞ –∏ —Å–æ —Å—Ç—Ä–µ—Å—Å–æ–º
    """
    print(f"\nüßÆ –ê–ù–ê–õ–ò–ó –†–ê–ó–õ–ò–ß–ò–ô –ù–ê –£–†–û–í–ù–ï –¢–†–ê–ô–õ–û–í")
    print("=" * 70)

    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    measures = [
        "blinks",
        "fixations",
        "fixation_duration_mean",
        "fixation_duration_median",
        "pupil_size",
        "runs",
        "saccade_amplitude_mean",
        "saccades",
        "trial_duration",
        "visited_areas",
    ]

    # –†—É—Å—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
    measure_names = {
        "blinks": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ—Ä–≥–∞–Ω–∏–π",
        "fixations": "–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–∫—Å–∞—Ü–∏–π",
        "fixation_duration_mean": "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ñ–∏–∫—Å–∞—Ü–∏–π (–º—Å)",
        "fixation_duration_median": "–ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ñ–∏–∫—Å–∞—Ü–∏–π (–º—Å)",
        "pupil_size": "–†–∞–∑–º–µ—Ä –∑—Ä–∞—á–∫–∞ (—Å—Ä–µ–¥–Ω–∏–π)",
        "runs": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–±–µ–≥–∞–Ω–∏–π –≤–∑–≥–ª—è–¥–∞",
        "saccade_amplitude_mean": "–°—Ä–µ–¥–Ω—è—è –∞–º–ø–ª–∏—Ç—É–¥–∞ —Å–∞–∫–∫–∞–¥ (¬∞)",
        "saccades": "–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞–∫–∫–∞–¥",
        "trial_duration": "–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–∞–π–ª–∞ (–º—Å)",
        "visited_areas": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö –∑–æ–Ω",
    }

    results = []
    p_values_for_correction = []

    # –î–∞–Ω–Ω—ã–µ –ø–æ —É—Å–ª–æ–≤–∏—è–º
    no_stress = trial_data[trial_data["condition"] == "no_stress"]
    stress = trial_data[trial_data["condition"] == "stress"]

    print(f"üìä –°–†–ê–í–ù–ï–ù–ò–ï –ì–†–£–ü–ü:")
    print(f"   –ë–µ–∑ —Å—Ç—Ä–µ—Å—Å–∞: {len(no_stress)} —Ç—Ä–∞–π–ª–∞")
    print(f"   –°–æ —Å—Ç—Ä–µ—Å—Å–æ–º: {len(stress)} —Ç—Ä–∞–π–ª–∞")

    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏
    total_n = len(no_stress) + len(stress)
    validate_sample_size(total_n, "–∞–Ω–∞–ª–∏–∑ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–π–ª–æ–≤")

    for measure in measures:
        if measure not in trial_data.columns:
            continue

        print(f"\nüìà {measure_names.get(measure, measure)}")
        print("-" * 50)

        no_stress_vals = no_stress[measure].dropna()
        stress_vals = stress[measure].dropna()

        # –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        ns_mean, ns_std = no_stress_vals.mean(), no_stress_vals.std()
        s_mean, s_std = stress_vals.mean(), stress_vals.std()

        print(f"–ë–µ–∑ —Å—Ç—Ä–µ—Å—Å–∞: M = {ns_mean:.2f} ¬± {ns_std:.2f}")
        print(f"–°–æ —Å—Ç—Ä–µ—Å—Å–æ–º:  M = {s_mean:.2f} ¬± {s_std:.2f}")

        # –ò–∑–º–µ–Ω–µ–Ω–∏–µ
        change = s_mean - ns_mean
        change_pct = (change / ns_mean) * 100 if ns_mean != 0 else 0
        change_symbol = "üìà" if change > 0 else "üìâ"
        print(f"{change_symbol} –ò–∑–º–µ–Ω–µ–Ω–∏–µ: {change:+.2f} ({change_pct:+.1f}%)")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º –æ –º–∞–ª–æ–π –≤—ã–±–æ—Ä–∫–µ
        if len(no_stress_vals) >= 3 and len(stress_vals) >= 3:
            try:
                stat, p_value = mannwhitneyu(
                    no_stress_vals, stress_vals, alternative="two-sided"
                )
                test_name = "–ö—Ä–∏—Ç–µ—Ä–∏–π –ú–∞–Ω–Ω–∞-–£–∏—Ç–Ω–∏"
            except:
                stat, p_value = ttest_ind(no_stress_vals, stress_vals)
                test_name = "t-–∫—Ä–∏—Ç–µ—Ä–∏–π"

            if p_value < ALPHA_LEVEL:
                significance = "‚úÖ H0 –û–¢–ö–õ–û–ù–Ø–ï–¢–°–Ø (p < 0.05)"
                hypothesis_result = "–†–∞–∑–ª–∏—á–∏—è –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò –ó–ù–ê–ß–ò–ú–´"
            else:
                significance = "üî∏ H0 –ù–ï –û–¢–ö–õ–û–ù–Ø–ï–¢–°–Ø (p ‚â• 0.05)"
                hypothesis_result = "–†–∞–∑–ª–∏—á–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –ù–ï –ó–ù–ê–ß–ò–ú–´"

            print(f"üß™ {test_name}: p = {p_value:.4f} - {significance}")
            print(f"‚öñÔ∏è –ó–∞–∫–ª—é—á–µ–Ω–∏–µ: {hypothesis_result}")

            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –º–∞–ª–æ–π –≤—ã–±–æ—Ä–∫–µ
            if p_value >= ALPHA_LEVEL:
                print(f"üö® –í–ù–ò–ú–ê–ù–ò–ï: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–∞–ª—ã–π —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ (N = {total_n})")
                print(f"   –ü—Ä–∏ —Ç–∞–∫–æ–π –º–∞–ª–æ–π –≤—ã–±–æ—Ä–∫–µ –ù–ï–í–û–ó–ú–û–ñ–ù–û –¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã:")
                print(f"   ‚Ä¢ –ù–ò –æ –Ω–∞–ª–∏—á–∏–∏ —Ä–∞–∑–ª–∏—á–∏–π (–µ—Å–ª–∏ p ‚â• 0.05)")
                print(f"   ‚Ä¢ –ù–ò –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ä–∞–∑–ª–∏—á–∏–π")

            p_values_for_correction.append(p_value)

            # –†–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞ —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏
            pooled_std = np.sqrt(
                (
                    (len(no_stress_vals) - 1) * ns_std**2
                    + (len(stress_vals) - 1) * s_std**2
                )
                / (len(no_stress_vals) + len(stress_vals) - 2)
            )
            cohens_d = (s_mean - ns_mean) / pooled_std if pooled_std > 0 else 0

            # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏
            effect_interpretation, warnings = interpret_effect_size_with_warnings(
                cohens_d, p_value, len(no_stress_vals) + len(stress_vals), measure
            )
            print(effect_interpretation)

        else:
            p_value = np.nan
            test_name = "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö"
            cohens_d = 0
            warnings = ["–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–∞–ª–∞—è –≤—ã–±–æ—Ä–∫–∞"]
            p_values_for_correction.append(np.nan)

        results.append(
            {
                "measure": measure,
                "measure_name": measure_names.get(measure, measure),
                "no_stress_mean": ns_mean,
                "no_stress_std": ns_std,
                "stress_mean": s_mean,
                "stress_std": s_std,
                "change_absolute": change,
                "change_percent": change_pct,
                "p_value": p_value,
                "cohens_d": cohens_d,
                "significant": p_value < ALPHA_LEVEL
                if not np.isnan(p_value)
                else False,
                "reliability_warnings": warnings if "warnings" in locals() else [],
            }
        )

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ–ø—Ä–∞–≤–∫—É –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    if len(p_values_for_correction) > 1:
        print(f"\nüîß –ü–û–ü–†–ê–í–ö–ê –ù–ê –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–´–ï –°–†–ê–í–ù–ï–ù–ò–Ø:")
        valid_p_values = [p for p in p_values_for_correction if not np.isnan(p)]
        if valid_p_values:
            corrected_p_values, corrected_alpha = apply_bonferroni_correction(
                valid_p_values
            )
            significant_after_correction = sum(
                1 for p in corrected_p_values if p < ALPHA_LEVEL
            )
            print(
                f"   –ü–æ—Å–ª–µ –ø–æ–ø—Ä–∞–≤–∫–∏ –ë–æ–Ω—Ñ–µ—Ä—Ä–æ–Ω–∏: {significant_after_correction}/{len(valid_p_values)} –∑–Ω–∞—á–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
            )

    return results


def analyze_trial_dynamics(trial_data):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É –∏–∑–º–µ–Ω–µ–Ω–∏–π –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Ç—Ä–∞–π–ª–∞–º
    """
    print(f"\nüìà –ê–ù–ê–õ–ò–ó –î–ò–ù–ê–ú–ò–ö–ò –ü–û –¢–†–ê–ô–õ–ê–ú")
    print("=" * 70)

    measures = [
        "blinks",
        "fixations",
        "fixation_duration_mean",
        "pupil_size",
        "runs",
        "saccade_amplitude_mean",
        "saccades",
        "visited_areas",
    ]

    measure_names = {
        "blinks": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ—Ä–≥–∞–Ω–∏–π",
        "fixations": "–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–∫—Å–∞—Ü–∏–π",
        "fixation_duration_mean": "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ñ–∏–∫—Å–∞—Ü–∏–π (–º—Å)",
        "pupil_size": "–†–∞–∑–º–µ—Ä –∑—Ä–∞—á–∫–∞",
        "runs": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–±–µ–≥–∞–Ω–∏–π",
        "saccade_amplitude_mean": "–ê–º–ø–ª–∏—Ç—É–¥–∞ —Å–∞–∫–∫–∞–¥ (¬∞)",
        "saccades": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞–∫–∫–∞–¥",
        "visited_areas": "–ü–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞ (%)",
    }

    dynamics = {}
    trial_stats = {}

    for measure in measures:
        if measure not in trial_data.columns:
            continue

        print(f"\nüìä {measure_names.get(measure, measure)}")
        print("-" * 40)

        # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —Ç—Ä–∞–π–ª–∞–º
        values = []
        phases = []
        trial_stats[measure] = {}

        for trial in range(1, MAX_TRIAL_NUMBER + 1):
            val = trial_data[trial_data["trial"] == trial][measure].iloc[0]
            phase = trial_data[trial_data["trial"] == trial]["phase"].iloc[0]
            values.append(val)
            phases.append(phase)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç—Ä–∞–π–ª–∞
            trial_stats[measure][trial] = {
                "mean": val,
                "std": 0,  # –£ –Ω–∞—Å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–∞–π–ª
                "phase": phase,
            }

            phase_emoji = {
                "baseline_1": "üìä",
                "baseline_2": "üìä",
                "baseline_3": "üìä",
                "stress_peak": "üî•",
                "stress_adapt": "üìâ",
                "stress_recovery": "üòå",
            }

            print(f"   {phase_emoji.get(phase, 'üìä')} –¢—Ä–∞–π–ª {trial}: {val:.2f}")

        # –ë–∞–∑–æ–≤–∞—è –ª–∏–Ω–∏—è
        baseline = np.mean(values[:3])

        # –ò–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–∞–∑–æ–≤–æ–π –ª–∏–Ω–∏–∏
        changes = [(v - baseline) / baseline * 100 for v in values]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω
        peak_trial = np.argmax(np.abs(changes)) + 1
        expected_peak_trial = STRESS_THRESHOLD + 1  # –ü–µ—Ä–≤—ã–π —Å—Ç—Ä–µ—Å—Å–æ–≤—ã–π —Ç—Ä–∞–π–ª

        if (
            peak_trial == expected_peak_trial
        ):  # –ü–∏–∫ –≤ –æ–∂–∏–¥–∞–µ–º–æ–º –ø–µ—Ä–≤–æ–º —Å—Ç—Ä–µ—Å—Å–æ–≤–æ–º —Ç—Ä–∞–π–ª–µ
            if len(changes) > expected_peak_trial and abs(changes[-1]) < abs(
                changes[expected_peak_trial - 1]
            ):  # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Ç—Ä–∞–π–ª—É
                pattern = f"üéØ –ü–ò–ö –≤ –¢{expected_peak_trial} ‚Üí –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï"
            else:
                pattern = f"üî• –ü–ò–ö –≤ –¢{expected_peak_trial} ‚Üí –ë–ï–ó –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–Ø"
        else:
            pattern = "‚ùì –ù–ï–°–¢–ê–ù–î–ê–†–¢–ù–´–ô –ü–ê–¢–¢–ï–†–ù"

        print(f"   üéØ –ü–∞—Ç—Ç–µ—Ä–Ω: {pattern}")
        print(f"   üìä –ë–∞–∑–æ–≤–∞—è –ª–∏–Ω–∏—è: {baseline:.2f}")
        print(
            f"   üìà –ú–∞–∫—Å. –∏–∑–º–µ–Ω–µ–Ω–∏–µ: {max(changes, key=abs):.1f}% (–¢—Ä–∞–π–ª {peak_trial})"
        )

        dynamics[measure] = {
            "values": values,
            "phases": phases,
            "changes": changes,
            "baseline": baseline,
            "pattern": pattern,
        }

    return dynamics, trial_stats


def analyze_word_dynamics(word_data, word_measure_names):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤ –ø–æ —Ç—Ä–∞–π–ª–∞–º
    """
    print(f"\nüìù –ê–ù–ê–õ–ò–ó –î–ò–ù–ê–ú–ò–ö–ò –°–õ–û–í –ü–û –¢–†–ê–ô–õ–ê–ú")
    print("=" * 70)

    measures = [
        "IA_FIRST_FIXATION_DURATION",
        "IA_FIXATION_COUNT",
        "IA_DWELL_TIME",
        "IA_DWELL_TIME_%",
        "IA_VISITED_TRIAL_%",
        "IA_REVISIT_TRIAL_%",
        "IA_RUN_COUNT",
    ]

    word_trial_stats = {}
    word_dynamics_results = {}

    for measure in measures:
        if measure not in word_data.columns:
            continue

        print(f"\nüìä {word_measure_names.get(measure, measure)}")
        print("-" * 50)

        word_trial_stats[measure] = {}

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞–∂–¥–æ–º—É —Ç—Ä–∞–π–ª—É
        trial_values = []
        for trial in range(1, MAX_TRIAL_NUMBER + 1):
            trial_data = word_data[word_data["trial"] == trial][measure].dropna()
            if len(trial_data) > 0:
                mean_val = trial_data.mean()
                std_val = trial_data.std()
                phase = word_data[word_data["trial"] == trial]["stress_phase"].iloc[0]
            else:
                mean_val = 0
                std_val = 0
                phase = f"trial_{trial}"

            word_trial_stats[measure][trial] = {
                "mean": mean_val,
                "std": std_val,
                "phase": phase,
                "count": len(trial_data),
            }
            trial_values.append(mean_val)

            print(
                f"   –¢—Ä–∞–π–ª {trial}: M = {mean_val:.2f} ¬± {std_val:.2f} (n = {len(trial_data)})"
            )

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã
        baseline_vs_peak_p = np.nan
        peak_vs_recovery_p = np.nan

        try:
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –ª–∏–Ω–∏–∏ —Å –ø–∏–∫–æ–º —Å—Ç—Ä–µ—Å—Å–∞ (–ø–µ—Ä–≤—ã–π —Å—Ç—Ä–µ—Å—Å–æ–≤—ã–π —Ç—Ä–∞–π–ª)
            baseline_data = []
            for t in range(1, STRESS_THRESHOLD + 1):
                baseline_data.extend(
                    word_data[word_data["trial"] == t][measure].dropna().tolist()
                )
            peak_trial = STRESS_THRESHOLD + 1  # –ü–µ—Ä–≤—ã–π —Å—Ç—Ä–µ—Å—Å–æ–≤—ã–π —Ç—Ä–∞–π–ª
            peak_data = (
                word_data[word_data["trial"] == peak_trial][measure].dropna().tolist()
            )

            if len(baseline_data) > 0 and len(peak_data) > 0:
                _, baseline_vs_peak_p = mannwhitneyu(
                    baseline_data, peak_data, alternative="two-sided"
                )

            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–∏–∫–∞ —Å—Ç—Ä–µ—Å—Å–∞ —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º (–ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç—Ä–∞–π–ª)
            recovery_data = (
                word_data[word_data["trial"] == MAX_TRIAL_NUMBER][measure]
                .dropna()
                .tolist()
            )
            if len(peak_data) > 0 and len(recovery_data) > 0:
                _, peak_vs_recovery_p = mannwhitneyu(
                    peak_data, recovery_data, alternative="two-sided"
                )

        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–º —Ç–µ—Å—Ç–µ: {e}")

        word_dynamics_results[measure] = {
            "baseline_vs_peak_p": baseline_vs_peak_p,
            "peak_vs_recovery_p": peak_vs_recovery_p,
            "measure": measure,
        }

        if not np.isnan(baseline_vs_peak_p):
            print(f"   üß™ –ë–∞–∑–∞ vs –¢{peak_trial}: p = {baseline_vs_peak_p:.4f}")
        if not np.isnan(peak_vs_recovery_p):
            print(
                f"   üß™ –¢{peak_trial} vs –¢{MAX_TRIAL_NUMBER}: p = {peak_vs_recovery_p:.4f}"
            )

    return word_trial_stats, word_dynamics_results


def create_enhanced_word_visualizations(
    word_data, word_test_results, word_measure_names
):
    """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤"""
    print(f"\nüé® –°–û–ó–î–ê–ù–ò–ï –ì–†–ê–§–ò–ö–û–í –ê–ù–ê–õ–ò–ó–ê –°–õ–û–í")
    print("=" * 50)

    measures = [r["measure"] for r in word_test_results]

    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É
    fig, axes = plt.subplots(3, 3, figsize=(20, 16))
    axes = axes.flatten()

    colors_no_stress = "#2E86C1"  # –°–∏–Ω–∏–π
    colors_stress = "#E74C3C"  # –ö—Ä–∞—Å–Ω—ã–π

    for i, result in enumerate(word_test_results):
        if i >= len(axes):
            break

        ax = axes[i]
        measure = result["measure"]

        # –î–∞–Ω–Ω—ã–µ –¥–ª—è boxplot
        no_stress_data = word_data[word_data["condition"] == "no_stress"][
            measure
        ].dropna()
        stress_data = word_data[word_data["condition"] == "stress"][measure].dropna()

        if len(no_stress_data) == 0 or len(stress_data) == 0:
            ax.text(
                0.5,
                0.5,
                "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ\n–¥–∞–Ω–Ω—ã—Ö",
                ha="center",
                va="center",
                transform=ax.transAxes,
            )
            ax.set_title(f"{result['measure_name']}", fontweight="bold", fontsize=11)
            continue

        # –°–æ–∑–¥–∞–µ–º boxplot
        bp = ax.boxplot(
            [no_stress_data, stress_data],
            labels=["–ë–µ–∑ —Å—Ç—Ä–µ—Å—Å–∞", "–°–æ —Å—Ç—Ä–µ—Å—Å–æ–º"],
            patch_artist=True,
            notch=True,
            widths=0.6,
        )

        # –†–∞—Å–∫—Ä–∞—à–∏–≤–∞–µ–º –±–æ–∫—Å—ã
        bp["boxes"][0].set_facecolor(colors_no_stress)
        bp["boxes"][0].set_alpha(0.7)
        bp["boxes"][1].set_facecolor(colors_stress)
        bp["boxes"][1].set_alpha(0.7)

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è
        ax.set_title(f"{result['measure_name']}", fontweight="bold", fontsize=11)

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–µ–ª–∫—É –ø–æ–∫–∞–∑—ã–≤–∞—é—â—É—é –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        y_max = max(no_stress_data.max(), stress_data.max())
        y_min = min(no_stress_data.min(), stress_data.min())
        y_range = y_max - y_min

        # –ü–æ–∑–∏—Ü–∏—è –¥–ª—è —Å—Ç—Ä–µ–ª–∫–∏ –∏ —Ç–µ–∫—Å—Ç–∞
        arrow_y = y_max + y_range * 0.1
        text_y = y_max + y_range * 0.15

        if result["change_direction"] == "—É–≤–µ–ª–∏—á–µ–Ω–∏–µ":
            ax.annotate(
                "",
                xy=(2, arrow_y),
                xytext=(1, arrow_y),
                arrowprops=dict(arrowstyle="->", lw=2, color="red"),
            )
            direction_symbol = "‚ÜóÔ∏è"
        else:
            ax.annotate(
                "",
                xy=(1, arrow_y),
                xytext=(2, arrow_y),
                arrowprops=dict(arrowstyle="->", lw=2, color="blue"),
            )
            direction_symbol = "‚ÜòÔ∏è"

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏–∏
        change_text = f"{direction_symbol} {abs(result['change_percent']):.1f}%"
        ax.text(
            1.5,
            text_y,
            change_text,
            ha="center",
            va="bottom",
            fontweight="bold",
            fontsize=10,
        )

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
        if result["significant"]:
            significance_text = f"p = {result['p_value']:.3f} ‚úÖ"
            bbox_color = "lightgreen"
        else:
            significance_text = f"p = {result['p_value']:.3f} ‚ùå"
            bbox_color = "lightcoral"

        ax.text(
            0.5,
            0.02,
            significance_text,
            transform=ax.transAxes,
            ha="center",
            va="bottom",
            bbox=dict(boxstyle="round,pad=0.3", facecolor=bbox_color, alpha=0.7),
        )

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞–∫ —Ç–æ—á–∫–∏
        ax.scatter(
            [1],
            [result["mean_no_stress"]],
            color="darkblue",
            s=50,
            zorder=10,
            marker="D",
        )
        ax.scatter(
            [2], [result["mean_stress"]], color="darkred", s=50, zorder=10, marker="D"
        )

        ax.grid(True, alpha=0.3)
        ax.set_ylabel("–ó–Ω–∞—á–µ–Ω–∏–µ")

    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∏
    for i in range(len(word_test_results), len(axes)):
        fig.delaxes(axes[i])

    # –û–±—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
    fig.suptitle(
        "üìù –í–õ–ò–Ø–ù–ò–ï –°–¢–†–ï–°–°–ê –ù–ê –ü–ê–†–ê–ú–ï–¢–†–´ –ê–ô–¢–†–ï–ö–ò–ù–ì–ê –ü–†–ò –ß–¢–ï–ù–ò–ò (–£–†–û–í–ï–ù–¨ –°–õ–û–í)",
        fontsize=16,
        fontweight="bold",
        y=0.98,
    )

    plt.tight_layout()
    plt.subplots_adjust(top=0.93)
    plt.savefig(
        f"{RESULTS_DIR}/word_level_stress_analysis.png",
        dpi=FIGURE_DPI,
        bbox_inches="tight",
    )
    show_plot_conditionally()


def create_dynamics_visualizations(
    trial_data,
    trial_stats,
    word_data,
    word_trial_stats,
    word_dynamics_results,
    word_measure_names,
):
    """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –¥–∏–Ω–∞–º–∏–∫–∏ —Å—Ç—Ä–µ—Å—Å–∞ –ø–æ —Ç—Ä–∞–π–ª–∞–º –¥–ª—è –æ–±–æ–∏—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
    print(f"\nüé® –°–û–ó–î–ê–ù–ò–ï –ì–†–ê–§–ò–ö–û–í –î–ò–ù–ê–ú–ò–ö–ò –ü–û –¢–†–ê–ô–õ–ê–ú")
    print("=" * 50)

    # 1. –î–ò–ù–ê–ú–ò–ö–ê –ù–ê –£–†–û–í–ù–ï –¢–†–ê–ô–õ–û–í
    print("üìä –ì—Ä–∞—Ñ–∏–∫–∏ –¥–∏–Ω–∞–º–∏–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–π–ª–æ–≤...")

    trial_measures = list(trial_stats.keys())

    # –°–æ–∑–¥–∞–µ–º –±–æ–ª—å—à—É—é —Ñ–∏–≥—É—Ä—É –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–π–ª–æ–≤
    fig1, axes1 = plt.subplots(3, 3, figsize=(24, 18))
    axes1 = axes1.flatten()

    # –¶–≤–µ—Ç–∞ –¥–ª—è —Ñ–∞–∑
    phase_colors = {
        "baseline_1": "#3498DB",  # –°–∏–Ω–∏–π
        "baseline_2": "#5DADE2",  # –°–≤–µ—Ç–ª–æ-—Å–∏–Ω–∏–π
        "baseline_3": "#85C1E9",  # –ï—â–µ —Å–≤–µ—Ç–ª–µ–µ —Å–∏–Ω–∏–π
        "stress_peak": "#E74C3C",  # –ö—Ä–∞—Å–Ω—ã–π (–ø–∏–∫)
        "stress_adapt": "#F1948A",  # –†–æ–∑–æ–≤—ã–π (–∞–¥–∞–ø—Ç–∞—Ü–∏—è)
        "stress_recovery": "#F8C471",  # –ñ–µ–ª—Ç—ã–π (–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ)
    }

    for i, measure in enumerate(trial_measures):
        if i >= len(axes1):
            break

        ax = axes1[i]

        # –î–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
        trials = sorted(trial_stats[measure].keys())
        means = [trial_stats[measure][t]["mean"] for t in trials]
        stds = [trial_stats[measure][t]["std"] for t in trials]
        phases = [trial_stats[measure][t]["phase"] for t in trials]
        colors = [get_phase_color(phase) for phase in phases]

        # –°–æ–∑–¥–∞–µ–º bar plot —Å —Ä–∞–∑–Ω—ã–º–∏ —Ü–≤–µ—Ç–∞–º–∏ –¥–ª—è —Ñ–∞–∑
        bars = ax.bar(
            trials,
            means,
            yerr=stds,
            capsize=5,
            color=colors,
            alpha=0.8,
            edgecolor="black",
            linewidth=1,
        )

        # –î–æ–±–∞–≤–ª—è–µ–º –ª–∏–Ω–∏—é —Ç—Ä–µ–Ω–¥–∞
        ax.plot(trials, means, "k--", alpha=0.7, linewidth=2, marker="o", markersize=6)

        # –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è —Ä–∞–∑–¥–µ–ª—è—é—â–∞—è —Ñ–∞–∑—ã
        ax.axvline(x=3.5, color="red", linestyle=":", linewidth=2, alpha=0.7)
        ax.text(
            3.5,
            max(means) * 0.9,
            "–ù–ê–ß–ê–õ–û\n–°–¢–†–ï–°–°–ê",
            ha="center",
            va="center",
            bbox=dict(boxstyle="round,pad=0.3", facecolor="red", alpha=0.3),
        )

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        trial_measure_names = {
            "blinks": "–ú–æ—Ä–≥–∞–Ω–∏—è",
            "fixations": "–§–∏–∫—Å–∞—Ü–∏–∏",
            "fixation_duration_mean": "–î–ª–∏—Ç. —Ñ–∏–∫—Å–∞—Ü–∏–π",
            "pupil_size": "–†–∞–∑–º–µ—Ä –∑—Ä–∞—á–∫–∞",
            "runs": "–ó–∞–±–µ–≥–∞–Ω–∏—è",
            "saccade_amplitude_mean": "–ê–º–ø–ª–∏—Ç—É–¥–∞ —Å–∞–∫–∫–∞–¥",
            "saccades": "–°–∞–∫–∫–∞–¥—ã",
            "visited_areas": "–ü–æ—Å–µ—â. –∑–æ–Ω—ã",
        }
        ax.set_title(
            f"{trial_measure_names.get(measure, measure)}",
            fontweight="bold",
            fontsize=13,
        )

        # –ü–æ–¥–ø–∏—Å–∏ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ –±–∞—Ä–∞—Ö
        baseline_mean = np.mean([trial_stats[measure][t]["mean"] for t in [1, 2, 3]])
        for j, (trial, mean_val) in enumerate(zip(trials, means)):
            if trial > 3:  # –¢–æ–ª—å–∫–æ –¥–ª—è —Å—Ç—Ä–µ—Å—Å–æ–≤—ã—Ö —Ç—Ä–∞–π–ª–æ–≤
                change_pct = ((mean_val - baseline_mean) / baseline_mean) * 100
                symbol = "‚Üó" if change_pct > 0 else "‚Üò"
                ax.text(
                    trial,
                    mean_val + stds[j] + max(means) * 0.02,
                    f"{symbol}{abs(change_pct):.1f}%",
                    ha="center",
                    va="bottom",
                    fontweight="bold",
                    fontsize=10,
                )

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Å–µ–π
        ax.set_xlabel("–ù–æ–º–µ—Ä —Ç—Ä–∞–π–ª–∞")
        ax.set_ylabel("–ó–Ω–∞—á–µ–Ω–∏–µ")
        ax.set_xticks(trials)
        ax.grid(True, alpha=0.3)

        # –î–æ–±–∞–≤–ª—è–µ–º –ª–µ–≥–µ–Ω–¥—É —Ñ–∞–∑ (—Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–≤–æ–º –≥—Ä–∞—Ñ–∏–∫–µ)
        if i == 0:
            legend_elements = []
            legend_labels = {
                "baseline_1": "–ë–∞–∑–æ–≤–∞—è –ª–∏–Ω–∏—è",
                "stress_peak": "–ü–∏–∫ —Å—Ç—Ä–µ—Å—Å–∞",
                "stress_adapt": "–ê–¥–∞–ø—Ç–∞—Ü–∏—è",
                "stress_recovery": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ",
            }
            for phase, color in phase_colors.items():
                if phase in legend_labels:
                    legend_elements.append(
                        plt.Rectangle((0, 0), 1, 1, facecolor=color, alpha=0.8)
                    )

            ax.legend(
                legend_elements,
                list(legend_labels.values()),
                loc="upper right",
                fontsize=9,
            )

    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∏
    for i in range(len(trial_measures), len(axes1)):
        fig1.delaxes(axes1[i])

    # –û–±—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
    fig1.suptitle(
        "üìä –î–ò–ù–ê–ú–ò–ö–ê –°–¢–†–ï–°–°–ê –ü–û –¢–†–ê–ô–õ–ê–ú (–£–†–û–í–ï–ù–¨ –¢–†–ê–ô–õ–û–í)",
        fontsize=18,
        fontweight="bold",
        y=0.98,
    )

    plt.tight_layout()
    plt.subplots_adjust(top=0.92)
    plt.savefig(
        f"{RESULTS_DIR}/trial_level_dynamics.png", dpi=FIGURE_DPI, bbox_inches="tight"
    )
    show_plot_conditionally()

    # 2. –î–ò–ù–ê–ú–ò–ö–ê –ù–ê –£–†–û–í–ù–ï –°–õ–û–í
    print("üìù –ì—Ä–∞—Ñ–∏–∫–∏ –¥–∏–Ω–∞–º–∏–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤...")

    word_measures = list(word_trial_stats.keys())[:8]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 8 –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã

    # –°–æ–∑–¥–∞–µ–º –±–æ–ª—å—à—É—é —Ñ–∏–≥—É—Ä—É –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
    fig2, axes2 = plt.subplots(3, 3, figsize=(24, 18))
    axes2 = axes2.flatten()

    for i, measure in enumerate(word_measures):
        if i >= len(axes2):
            break

        ax = axes2[i]

        # –î–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
        trials = sorted(word_trial_stats[measure].keys())
        means = [word_trial_stats[measure][t]["mean"] for t in trials]
        stds = [word_trial_stats[measure][t]["std"] for t in trials]
        phases = [word_trial_stats[measure][t]["phase"] for t in trials]
        colors = [get_phase_color(phase) for phase in phases]

        # –°–æ–∑–¥–∞–µ–º bar plot —Å error bars
        bars = ax.bar(
            trials,
            means,
            yerr=stds,
            capsize=5,
            color=colors,
            alpha=0.8,
            edgecolor="black",
            linewidth=1,
        )

        # –î–æ–±–∞–≤–ª—è–µ–º –ª–∏–Ω–∏—é —Ç—Ä–µ–Ω–¥–∞
        ax.plot(trials, means, "k--", alpha=0.7, linewidth=2, marker="o", markersize=6)

        # –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è —Ä–∞–∑–¥–µ–ª—è—é—â–∞—è —Ñ–∞–∑—ã
        ax.axvline(x=3.5, color="red", linestyle=":", linewidth=2, alpha=0.7)
        ax.text(
            3.5,
            max(means) * 0.9,
            "–ù–ê–ß–ê–õ–û\n–°–¢–†–ï–°–°–ê",
            ha="center",
            va="center",
            bbox=dict(boxstyle="round,pad=0.3", facecolor="red", alpha=0.3),
        )

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        ax.set_title(
            f"{word_measure_names.get(measure, measure)}",
            fontweight="bold",
            fontsize=11,
        )

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å—Ä–∞–≤–Ω–µ–Ω–∏–π
        result = word_dynamics_results.get(measure)
        if result:
            significance_text = ""
            peak_trial_num = STRESS_THRESHOLD + 1
            if (
                not np.isnan(result["baseline_vs_peak_p"])
                and result["baseline_vs_peak_p"] < ALPHA_LEVEL
            ):
                significance_text += f"–ë–∞–∑–∞‚Üî{peak_trial_num}: ‚úÖ "
            if (
                not np.isnan(result["peak_vs_recovery_p"])
                and result["peak_vs_recovery_p"] < ALPHA_LEVEL
            ):
                significance_text += f"{peak_trial_num}‚Üî{MAX_TRIAL_NUMBER}: ‚úÖ"

            if significance_text:
                ax.text(
                    0.02,
                    0.98,
                    significance_text,
                    transform=ax.transAxes,
                    va="top",
                    ha="left",
                    fontsize=9,
                    bbox=dict(
                        boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.7
                    ),
                )

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Å–µ–π
        ax.set_xlabel("–ù–æ–º–µ—Ä —Ç—Ä–∞–π–ª–∞")
        ax.set_ylabel("–ó–Ω–∞—á–µ–Ω–∏–µ")
        ax.set_xticks(trials)
        ax.grid(True, alpha=0.3)

    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∏
    for i in range(len(word_measures), len(axes2)):
        fig2.delaxes(axes2[i])

    # –û–±—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
    fig2.suptitle(
        "üìù –î–ò–ù–ê–ú–ò–ö–ê –°–¢–†–ï–°–°–ê –ü–û –¢–†–ê–ô–õ–ê–ú (–£–†–û–í–ï–ù–¨ –°–õ–û–í)",
        fontsize=18,
        fontweight="bold",
        y=0.98,
    )

    plt.tight_layout()
    plt.subplots_adjust(top=0.92)
    plt.savefig(
        f"{RESULTS_DIR}/word_level_dynamics.png", dpi=FIGURE_DPI, bbox_inches="tight"
    )
    show_plot_conditionally()


def create_comprehensive_visualizations(trial_data, comparison_results, dynamics):
    """
    –°–æ–∑–¥–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∞–π—Ç—Ä–µ–∫–∏–Ω–≥–∞
    """
    print(f"\nüé® –°–û–ó–î–ê–ù–ò–ï –ö–û–ú–ü–õ–ï–ö–°–ù–´–• –ì–†–ê–§–ò–ö–û–í")
    print("=" * 50)

    # 1. –°–†–ê–í–ù–ï–ù–ò–ï –£–°–õ–û–í–ò–ô (–ë–ï–ó –°–¢–†–ï–°–°–ê VS –°–û –°–¢–†–ï–°–°–û–ú)
    fig, axes = plt.subplots(2, 5, figsize=(25, 12))
    axes = axes.flatten()

    for i, result in enumerate(comparison_results[:10]):
        if i >= len(axes):
            break

        ax = axes[i]
        measure = result["measure"]

        # –î–∞–Ω–Ω—ã–µ –¥–ª—è boxplot
        no_stress_data = trial_data[trial_data["condition"] == "no_stress"][measure]
        stress_data = trial_data[trial_data["condition"] == "stress"][measure]

        # –°–æ–∑–¥–∞–µ–º boxplot
        bp = ax.boxplot(
            [no_stress_data, stress_data],
            labels=[
                f"–ë–µ–∑ —Å—Ç—Ä–µ—Å—Å–∞\n(–¢1-{STRESS_THRESHOLD})",
                f"–°–æ —Å—Ç—Ä–µ—Å—Å–æ–º\n(–¢{STRESS_THRESHOLD + 1}-{MAX_TRIAL_NUMBER})",
            ],
            patch_artist=True,
            widths=0.6,
        )

        # –†–∞—Å–∫—Ä–∞—à–∏–≤–∞–µ–º
        bp["boxes"][0].set_facecolor(COLORS_NO_STRESS)
        bp["boxes"][0].set_alpha(0.7)
        bp["boxes"][1].set_facecolor(COLORS_STRESS)
        bp["boxes"][1].set_alpha(0.7)

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        ax.set_title(f"{result['measure_name']}", fontweight="bold", fontsize=11)

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        change_pct = result["change_percent"]
        symbol = "‚Üó" if change_pct > 0 else "‚Üò"
        ax.text(
            0.5,
            0.95,
            f"{symbol}{abs(change_pct):.1f}%",
            transform=ax.transAxes,
            ha="center",
            va="top",
            fontweight="bold",
            fontsize=12,
        )

        # –ó–Ω–∞—á–∏–º–æ—Å—Ç—å
        if result["significant"]:
            significance_text = f"p = {result['p_value']:.3f} ‚úÖ"
            bbox_color = "lightgreen"
        else:
            significance_text = (
                f"p = {result['p_value']:.3f}"
                if not np.isnan(result["p_value"])
                else "n.s."
            )
            bbox_color = "lightgray"

        ax.text(
            0.5,
            0.02,
            significance_text,
            transform=ax.transAxes,
            ha="center",
            va="bottom",
            fontsize=9,
            bbox=dict(boxstyle="round,pad=0.3", facecolor=bbox_color, alpha=0.7),
        )

        ax.grid(True, alpha=0.3)

    plt.suptitle(
        "üß† –°–†–ê–í–ù–ï–ù–ò–ï –ü–û–ö–ê–ó–ê–¢–ï–õ–ï–ô –ê–ô–¢–†–ï–ö–ò–ù–ì–ê: –ë–ï–ó –°–¢–†–ï–°–°–ê vs –°–û –°–¢–†–ï–°–°–û–ú",
        fontsize=16,
        fontweight="bold",
    )
    plt.tight_layout()
    plt.subplots_adjust(top=0.9)
    plt.savefig(
        f"{RESULTS_DIR}/comprehensive_stress_comparison.png",
        dpi=FIGURE_DPI,
        bbox_inches="tight",
    )
    show_plot_conditionally()

    # 2. –î–ò–ù–ê–ú–ò–ö–ê –ü–û –¢–†–ê–ô–õ–ê–ú
    fig, axes = plt.subplots(2, 4, figsize=(20, 12))
    axes = axes.flatten()

    measures_for_dynamics = list(dynamics.keys())[:8]

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–ª—É—á–µ–Ω–∏—è —Ü–≤–µ—Ç–æ–≤ —Ñ–∞–∑
    for i, measure in enumerate(measures_for_dynamics):
        ax = axes[i]

        values = dynamics[measure]["values"]
        phases = dynamics[measure]["phases"]

        # –°–æ–∑–¥–∞–µ–º –ª–∏–Ω–µ–π–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫
        trials = list(range(1, 7))
        ax.plot(trials, values, "o-", linewidth=3, markersize=8, color="darkblue")

        # –†–∞—Å–∫—Ä–∞—à–∏–≤–∞–µ–º —Ç–æ—á–∫–∏ –ø–æ —Ñ–∞–∑–∞–º (–∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Ñ—É–Ω–∫—Ü–∏—é)
        for j, (trial, value, phase) in enumerate(zip(trials, values, phases)):
            ax.scatter(
                trial,
                value,
                s=150,
                c=get_phase_color(phase),
                edgecolor="black",
                linewidth=2,
                zorder=5,
            )

        # –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è —Ä–∞–∑–¥–µ–ª—è—é—â–∞—è —Ñ–∞–∑—ã
        ax.axvline(x=3.5, color="red", linestyle="--", alpha=0.7, linewidth=2)
        ax.text(
            3.5,
            max(values) * 0.9,
            "–ù–ê–ß–ê–õ–û\n–°–¢–†–ï–°–°–ê",
            ha="center",
            va="center",
            bbox=dict(boxstyle="round,pad=0.3", facecolor="red", alpha=0.3),
        )

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º
        measure_name = {
            "blinks": "–ú–æ—Ä–≥–∞–Ω–∏—è",
            "fixations": "–§–∏–∫—Å–∞—Ü–∏–∏",
            "fixation_duration_mean": "–î–ª–∏—Ç. —Ñ–∏–∫—Å–∞—Ü–∏–π",
            "pupil_size": "–†–∞–∑–º–µ—Ä –∑—Ä–∞—á–∫–∞",
            "runs": "–ó–∞–±–µ–≥–∞–Ω–∏—è",
            "saccade_amplitude_mean": "–ê–º–ø–ª–∏—Ç—É–¥–∞ —Å–∞–∫–∫–∞–¥",
            "saccades": "–°–∞–∫–∫–∞–¥—ã",
            "visited_areas": "–ü–æ—Å–µ—â. –∑–æ–Ω—ã",
        }.get(measure, measure)

        ax.set_title(
            f"{measure_name}\n{dynamics[measure]['pattern']}",
            fontweight="bold",
            fontsize=10,
        )

        ax.set_xlabel("–ù–æ–º–µ—Ä —Ç—Ä–∞–π–ª–∞")
        ax.set_ylabel("–ó–Ω–∞—á–µ–Ω–∏–µ")
        ax.set_xticks(trials)
        ax.grid(True, alpha=0.3)

    plt.suptitle(
        "üìä –î–ò–ù–ê–ú–ò–ö–ê –ü–û–ö–ê–ó–ê–¢–ï–õ–ï–ô –ê–ô–¢–†–ï–ö–ò–ù–ì–ê –ü–û –¢–†–ê–ô–õ–ê–ú", fontsize=16, fontweight="bold"
    )
    plt.tight_layout()
    plt.subplots_adjust(top=0.9)
    plt.savefig(
        f"{RESULTS_DIR}/comprehensive_trial_dynamics.png",
        dpi=FIGURE_DPI,
        bbox_inches="tight",
    )
    show_plot_conditionally()

    # 3. –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ô –ì–†–ê–§–ò–ö: –ö–õ–Æ–ß–ï–í–´–ï –ú–ê–†–ö–ï–†–´ –°–¢–†–ï–°–°–ê
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))

    # –°–∞–º—ã–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
    key_measures = ["pupil_size", "blinks", "saccade_amplitude_mean"]
    key_names = [
        "–†–∞–∑–º–µ—Ä –∑—Ä–∞—á–∫–∞ (—Å—Ç—Ä–µ—Å—Å-–º–∞—Ä–∫–µ—Ä)",
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ—Ä–≥–∞–Ω–∏–π",
        "–ê–º–ø–ª–∏—Ç—É–¥–∞ —Å–∞–∫–∫–∞–¥",
    ]

    for i, (measure, name) in enumerate(zip(key_measures, key_names)):
        ax = axes[i]

        no_stress = trial_data[trial_data["condition"] == "no_stress"][measure]
        stress = trial_data[trial_data["condition"] == "stress"][measure]

        # Violin plot –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã
        violin = ax.violinplot([no_stress, stress], positions=[1, 2], widths=0.5)
        ax.scatter(
            [1] * len(no_stress),
            no_stress,
            alpha=0.7,
            s=100,
            color="blue",
            label="–ë–µ–∑ —Å—Ç—Ä–µ—Å—Å–∞",
        )
        ax.scatter(
            [2] * len(stress),
            stress,
            alpha=0.7,
            s=100,
            color="red",
            label="–°–æ —Å—Ç—Ä–µ—Å—Å–æ–º",
        )

        # –°—Ä–µ–¥–Ω–∏–µ –ª–∏–Ω–∏–∏
        ax.hlines(
            no_stress.mean(), 0.7, 1.3, colors="blue", linestyle="--", linewidth=2
        )
        ax.hlines(stress.mean(), 1.7, 2.3, colors="red", linestyle="--", linewidth=2)

        ax.set_title(name, fontweight="bold")
        ax.set_xticks([1, 2])
        ax.set_xticklabels(["–ë–µ–∑ —Å—Ç—Ä–µ—Å—Å–∞", "–°–æ —Å—Ç—Ä–µ—Å—Å–æ–º"])
        ax.grid(True, alpha=0.3)

        if i == 0:
            ax.legend()

    plt.suptitle(
        "üéØ –ö–õ–Æ–ß–ï–í–´–ï –ú–ê–†–ö–ï–†–´ –°–¢–†–ï–°–°–ê –í –î–ê–ù–ù–´–• –ê–ô–¢–†–ï–ö–ò–ù–ì–ê",
        fontsize=16,
        fontweight="bold",
    )
    plt.tight_layout()
    plt.subplots_adjust(top=0.85)
    plt.savefig(
        f"{RESULTS_DIR}/key_stress_markers.png", dpi=FIGURE_DPI, bbox_inches="tight"
    )
    show_plot_conditionally()


def test_formal_hypotheses(word_comparison_results, trial_comparison_results):
    """
    –ü—Ä–æ–≤–æ–¥–∏—Ç —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑ —Å –≤—ã–≤–æ–¥–∞–º–∏
    """
    print(f"\nüî¨ –§–û–†–ú–ê–õ–¨–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ì–ò–ü–û–¢–ï–ó")
    print("=" * 80)

    # –ü–æ–¥—Å—á–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    word_significant = [r for r in word_comparison_results if r["significant"]]
    trial_significant = [r for r in trial_comparison_results if r["significant"]]
    total_significant = len(word_significant) + len(trial_significant)
    total_tests = len(word_comparison_results) + len(trial_comparison_results)

    print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
    print(f"   ‚Ä¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {total_tests}")
    print(
        f"   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞—á–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (p < {ALPHA_LEVEL}): {total_significant}"
    )
    print(
        f"   ‚Ä¢ –ü—Ä–æ—Ü–µ–Ω—Ç –∑–Ω–∞—á–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {total_significant / total_tests * 100:.1f}%"
    )

    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏
    all_sample_sizes = []
    for result in trial_comparison_results:
        if "no_stress_mean" in result:  # trial results
            all_sample_sizes.append(TOTAL_TRIALS)  # Dynamic sample size for trials
    for result in word_comparison_results:
        if "mean_no_stress" in result:  # word results
            all_sample_sizes.append(
                548
            )  # Word level sample size (could be dynamic too)

    min_trial_sample = TOTAL_TRIALS  # Dynamic from loaded data
    if min_trial_sample < MIN_SAMPLE_SIZE_WARNING:
        print(f"\nüö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ú–ï–¢–û–î–û–õ–û–ì–ò–ß–ï–°–ö–û–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï:")
        print(f"   –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —Ç—Ä–∞–π–ª–æ–≤ (N={min_trial_sample}) –ö–†–ò–¢–ò–ß–ï–°–ö–ò –ú–ê–õ!")
        print(f"   –ü—Ä–∏ —Ç–∞–∫–æ–º —Ä–∞–∑–º–µ—Ä–µ –≤—ã–±–æ—Ä–∫–∏ –ù–ï–í–û–ó–ú–û–ñ–ù–û –¥–µ–ª–∞—Ç—å –Ω–∞–¥–µ–∂–Ω—ã–µ –≤—ã–≤–æ–¥—ã")

    print(f"\n‚öñÔ∏è –ü–†–û–í–ï–†–ö–ê –ì–ò–ü–û–¢–ï–ó –ü–û –£–†–û–í–ù–Ø–ú –ê–ù–ê–õ–ò–ó–ê:")

    # –ê–Ω–∞–ª–∏–∑ —É—Ä–æ–≤–Ω—è —Å–ª–æ–≤
    print(f"\n   üìù –£–†–û–í–ï–ù–¨ –°–õ–û–í (N = 548 –Ω–∞–±–ª—é–¥–µ–Ω–∏–π):")
    print(f"   ‚Ä¢ –ü—Ä–æ–≤–µ–¥–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤: {len(word_comparison_results)}")
    print(f"   ‚Ä¢ –ó–Ω–∞—á–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(word_significant)}")

    if len(word_significant) > 0:
        print(f"   üîπ –í–´–í–û–î: H0 –û–¢–ö–õ–û–ù–Ø–ï–¢–°–Ø –ø–æ {len(word_significant)} –ø–æ–∫–∞–∑–∞—Ç–µ–ª—é(—è–º)")
        for result in word_significant:
            print(
                f"      ‚úÖ {result['measure_name']}: p = {result['p_value']:.4f} < 0.05"
            )
    else:
        print(f"   üî∏ –í–´–í–û–î: H0 –ù–ï –û–¢–ö–õ–û–ù–Ø–ï–¢–°–Ø (–Ω–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π)")

    # –ê–Ω–∞–ª–∏–∑ —É—Ä–æ–≤–Ω—è —Ç—Ä–∞–π–ª–æ–≤
    print(f"\n   üìä –£–†–û–í–ï–ù–¨ –¢–†–ê–ô–õ–û–í (N = {TOTAL_TRIALS} —Ç—Ä–∞–π–ª–æ–≤):")
    print(f"   ‚Ä¢ –ü—Ä–æ–≤–µ–¥–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤: {len(trial_comparison_results)}")
    print(f"   ‚Ä¢ –ó–Ω–∞—á–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(trial_significant)}")

    if len(trial_significant) > 0:
        print(f"   üîπ –í–´–í–û–î: H0 –û–¢–ö–õ–û–ù–Ø–ï–¢–°–Ø –ø–æ {len(trial_significant)} –ø–æ–∫–∞–∑–∞—Ç–µ–ª—é(—è–º)")
        for result in trial_significant:
            print(
                f"      ‚úÖ {result['measure_name']}: p = {result['p_value']:.4f} < {ALPHA_LEVEL}"
            )
    else:
        print(f"   üî∏ –í–´–í–û–î: H0 –ù–ï –û–¢–ö–õ–û–ù–Ø–ï–¢–°–Ø (–Ω–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π)")
        print(f"   ‚ö†Ô∏è  –ü–†–ò–ß–ò–ù–ê: –ú–∞–ª—ã–π —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ (N = {TOTAL_TRIALS})")

        # –ù–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
        large_effects = [
            r
            for r in trial_comparison_results
            if abs(r["cohens_d"]) >= EFFECT_SIZE_LARGE
        ]
        print(
            f"   üìà –û–î–ù–ê–ö–û: –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(large_effects)} –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —Å –ë–û–õ–¨–®–ò–ú–ò —Ä–∞–∑–º–µ—Ä–∞–º–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞"
        )

    # –û–ë–©–ï–ï –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï –ü–û –ì–ò–ü–û–¢–ï–ó–ê–ú
    print(f"\nüèõÔ∏è –ò–¢–û–ì–û–í–û–ï –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï –ü–û –ì–ò–ü–û–¢–ï–ó–ê–ú:")

    if total_significant > 0:
        print(f"   ‚úÖ –ù–£–õ–ï–í–ê–Ø –ì–ò–ü–û–¢–ï–ó–ê H0 –ß–ê–°–¢–ò–ß–ù–û –û–¢–ö–õ–û–ù–Ø–ï–¢–°–Ø")
        print(f"   ‚úÖ –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–ê–Ø –ì–ò–ü–û–¢–ï–ó–ê H1 –ß–ê–°–¢–ò–ß–ù–û –ü–û–î–¢–í–ï–†–ñ–î–ê–ï–¢–°–Ø")
        print(f"   üéØ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï: –°—É—â–µ—Å—Ç–≤—É—é—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ —Ä–∞–∑–ª–∏—á–∏—è")
        print(f"      –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –∞–π—Ç—Ä–µ–∫–∏–Ω–≥–∞ –º–µ–∂–¥—É —É—Å–ª–æ–≤–∏—è–º–∏ —Å—Ç—Ä–µ—Å—Å–∞")
    else:
        print(f"   üî∏ –ù–£–õ–ï–í–ê–Ø –ì–ò–ü–û–¢–ï–ó–ê H0 –ù–ï –û–¢–ö–õ–û–ù–Ø–ï–¢–°–Ø –Ω–∞ —É—Ä–æ–≤–Ω–µ p < {ALPHA_LEVEL}")

        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ß–µ—Å—Ç–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –ø—Ä–∏ –º–∞–ª–æ–π –≤—ã–±–æ—Ä–∫–µ
        min_sample = min(TOTAL_TRIALS, 548)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å—Ä–µ–¥–∏ –∞–Ω–∞–ª–∏–∑–æ–≤
        if min_sample < MIN_SAMPLE_SIZE_WARNING:
            print(f"\n   üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:")
            print(
                f"   ‚Ä¢ –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —Ç—Ä–∞–π–ª–æ–≤ (N={TOTAL_TRIALS}) –ù–ï–î–û–°–¢–ê–¢–û–ß–ï–ù –¥–ª—è –≤—ã–≤–æ–¥–æ–≤"
            )
            print(f"   ‚Ä¢ –ù–ï–í–û–ó–ú–û–ñ–ù–û —É—Ç–≤–µ—Ä–∂–¥–∞—Ç—å –Ω–∞–ª–∏—á–∏–µ –ò–õ–ò –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∞")
            print(
                f"   ‚Ä¢ Cohen's d –ø—Ä–∏ –Ω–µ–∑–Ω–∞—á–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –∏ –º–∞–ª–æ–π –≤—ã–±–æ—Ä–∫–µ –ù–ï–ù–ê–î–ï–ñ–ï–ù"
            )
            print(f"   ‚Ä¢ –¢—Ä–µ–±—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏ –¥–æ N ‚â• {MIN_SAMPLE_SIZE_WARNING}")

            print(f"\n   üìã –ù–ê–£–ß–ù–û –û–ë–û–°–ù–û–í–ê–ù–ù–û–ï –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï:")
            print(f"   ‚Ä¢ –î–∞–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —è–≤–ª—è–µ—Ç—Å—è –ü–ò–õ–û–¢–ù–´–ú")
            print(f"   ‚Ä¢ –û—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –æ—Ç—Ä–∞–±–æ—Ç–∫–∞ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏")
            print(f"   ‚Ä¢ –ù–ï –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å—Ç—Ä–µ—Å—Å–∞")
            print(f"   ‚Ä¢ –ù–ï –æ–ø—Ä–æ–≤–µ—Ä–≥–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å—Ç—Ä–µ—Å—Å–∞")
        else:
            # –ü–æ–¥—Å—á–µ—Ç –±–æ–ª—å—à–∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
            all_large_effects = [
                r
                for r in word_comparison_results + trial_comparison_results
                if abs(r["cohens_d"]) >= EFFECT_SIZE_LARGE
            ]
            print(
                f"   üìä –ü–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —Å –±–æ–ª—å—à–∏–º —Ä–∞–∑–º–µ—Ä–æ–º —ç—Ñ—Ñ–µ–∫—Ç–∞: {len(all_large_effects)}"
            )
            print(f"   üî¨ –ü—Ä–∏ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ —Ä–∞–∑–ª–∏—á–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–∞–ª—å–Ω–µ–π—à–∏–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º
    print(f"\nüìã –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –î–ê–õ–¨–ù–ï–ô–®–ò–• –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ô:")

    if total_significant == 0:
        print(f"   1. –£–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–æ N ‚â• 30 —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤")
        print(f"   2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ —Å—Ç—Ä–µ—Å—Å–æ–≤—ã–µ —Å—Ç–∏–º—É–ª—ã")
        print(f"   3. –ü—Ä–æ–≤–µ—Å—Ç–∏ power-–∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–π –≤—ã–±–æ—Ä–∫–∏")

    print(f"   4. –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ü–∏—é –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (Bonferroni)")
    print(f"   5. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–æ–∫")
    print(f"   6. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –≤ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å—Ç—Ä–µ—Å—Å–∞")

    return total_significant, total_tests


def generate_comprehensive_report(
    word_comparison_results, trial_comparison_results, dynamics
):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å—Ç—Ä–µ—Å—Å–∞ —á–µ—Ä–µ–∑ –∞–π—Ç—Ä–µ–∫–∏–Ω–≥
    """
    print(f"\nüèÜ –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢: –ü–ò–õ–û–¢–ù–û–ï –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –ê–ô–¢–†–ï–ö–ò–ù–ì–ê –ò –°–¢–†–ï–°–°–ê")
    print("=" * 80)
    print(f"‚ö†Ô∏è –í–ê–ñ–ù–û: –î–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–ª—É—á–µ–Ω—ã –Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–∞–ª–æ–π –≤—ã–±–æ—Ä–∫–µ")
    print(f"   –∏ –ù–ï –º–æ–≥—É—Ç —Å–ª—É–∂–∏—Ç—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å—Ç—Ä–µ—Å—Å–∞!")

    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤
    word_significant_results = [r for r in word_comparison_results if r["significant"]]
    word_large_effects = [
        r for r in word_comparison_results if abs(r["cohens_d"]) >= 0.8
    ]
    word_medium_effects = [
        r for r in word_comparison_results if 0.5 <= abs(r["cohens_d"]) < 0.8
    ]

    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–π–ª–æ–≤
    trial_significant_results = [
        r for r in trial_comparison_results if r["significant"]
    ]
    trial_large_effects = [
        r for r in trial_comparison_results if abs(r["cohens_d"]) >= 0.8
    ]
    trial_medium_effects = [
        r for r in trial_comparison_results if 0.5 <= abs(r["cohens_d"]) < 0.8
    ]

    print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó:")
    print(f"\n   üìù –£–†–û–í–ï–ù–¨ –°–õ–û–í (N = 548 –Ω–∞–±–ª—é–¥–µ–Ω–∏–π):")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π: {len(word_comparison_results)}")
    print(f"   ‚Ä¢ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π: {len(word_significant_results)}")
    print(f"   ‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç—ã –±–æ–ª—å—à–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞: {len(word_large_effects)}")
    print(f"   ‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç—ã —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞: {len(word_medium_effects)}")

    print(f"\n   üìä –£–†–û–í–ï–ù–¨ –¢–†–ê–ô–õ–û–í (N = 6 —Ç—Ä–∞–π–ª–æ–≤):")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π: {len(trial_comparison_results)}")
    print(f"   ‚Ä¢ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π: {len(trial_significant_results)}")
    print(f"   ‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç—ã –±–æ–ª—å—à–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞: {len(trial_large_effects)}")
    print(f"   ‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç—ã —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞: {len(trial_medium_effects)}")

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–∞–º—ã—Ö –∑–Ω–∞—á–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    all_significant = word_significant_results + trial_significant_results
    all_large_effects = word_large_effects + trial_large_effects

    if all_significant:
        print(f"\n‚úÖ –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò –ó–ù–ê–ß–ò–ú–´–ï –†–ê–ó–õ–ò–ß–ò–Ø:")
        for result in all_significant:
            if "change_absolute" in result:  # –¢—Ä–∞–π–ª—ã
                direction = "–≤—ã—à–µ" if result["change_absolute"] > 0 else "–Ω–∏–∂–µ"
                print(
                    f"   üéØ {result['measure_name']}: {direction} –Ω–∞ {abs(result['change_percent']):.1f}%"
                )
                print(
                    f"      p = {result['p_value']:.3f}, Cohen's d = {result['cohens_d']:.3f}"
                )
            else:  # –°–ª–æ–≤–∞
                direction = result["change_direction"]
                print(
                    f"   üéØ {result['measure_name']}: {direction} –Ω–∞ {abs(result['change_percent']):.1f}%"
                )
                print(
                    f"      p = {result['p_value']:.3f}, Cohen's d = {result['cohens_d']:.3f}"
                )

    print(f"\nüìà –ù–ê–ë–õ–Æ–î–ê–ï–ú–´–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø –ü–û–î –í–õ–ò–Ø–ù–ò–ï–ú –°–¢–†–ï–°–°–ê:")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
    all_increases = []
    all_decreases = []

    for result in word_comparison_results + trial_comparison_results:
        if "change_absolute" in result:  # –¢—Ä–∞–π–ª—ã
            if result["change_absolute"] > 0:
                all_increases.append(result)
            else:
                all_decreases.append(result)
        else:  # –°–ª–æ–≤–∞
            if result["change_direction"] == "—É–≤–µ–ª–∏—á–µ–Ω–∏–µ":
                all_increases.append(result)
            else:
                all_decreases.append(result)

    if all_increases:
        print(f"\n   ‚¨ÜÔ∏è –£–í–ï–õ–ò–ß–ï–ù–ò–ï:")
        for result in sorted(
            all_increases, key=lambda x: abs(x["change_percent"]), reverse=True
        )[:10]:
            significance = " ‚úÖ" if result["significant"] else ""
            print(
                f"      ‚Ä¢ {result['measure_name']}: +{abs(result['change_percent']):.1f}%{significance}"
            )

    if all_decreases:
        print(f"\n   ‚¨áÔ∏è –£–ú–ï–ù–¨–®–ï–ù–ò–ï:")
        for result in sorted(
            all_decreases, key=lambda x: abs(x["change_percent"]), reverse=True
        )[:10]:
            significance = " ‚úÖ" if result["significant"] else ""
            print(
                f"      ‚Ä¢ {result['measure_name']}: {result['change_percent']:.1f}%{significance}"
            )

    # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–∏–Ω–∞–º–∏–∫–∏
    print(f"\nüîÑ –ê–ù–ê–õ–ò–ó –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–• –ü–ê–¢–¢–ï–†–ù–û–í:")

    peak_patterns = 0
    recovery_patterns = 0

    for measure, data in dynamics.items():
        if "–ü–ò–ö –≤ –¢4" in data["pattern"]:
            peak_patterns += 1
            if "–í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï" in data["pattern"]:
                recovery_patterns += 1

    print(f"   ‚Ä¢ –ü–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —Å –ø–∏–∫–æ–º –≤ —Ç—Ä–∞–π–ª–µ 4: {peak_patterns}/{len(dynamics)}")
    print(f"   ‚Ä¢ –ü–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º: {recovery_patterns}/{len(dynamics)}")

    # –ß–ï–°–¢–ù–´–ï –ö–õ–Æ–ß–ï–í–´–ï –í–´–í–û–î–´
    print(f"\nüß† –ö–õ–Æ–ß–ï–í–´–ï –í–´–í–û–î–´ –ü–ò–õ–û–¢–ù–û–ì–û –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:")

    total_significant = len(all_significant)
    total_large_effects = len(all_large_effects)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
    min_sample_size = TOTAL_TRIALS  # –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–π–ª–æ–≤

    if min_sample_size < MIN_SAMPLE_SIZE_WARNING:
        print(f"   üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï:")
        print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ (N={min_sample_size}) –ö–†–ò–¢–ò–ß–ï–°–ö–ò –ú–ê–õ")
        print(f"   ‚Ä¢ –ù–ï–í–û–ó–ú–û–ñ–ù–û —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã –æ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å—Ç—Ä–µ—Å—Å–∞")
        print(f"   ‚Ä¢ –î–∞–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —è–≤–ª—è–µ—Ç—Å—è –ü–ò–õ–û–¢–ù–´–ú")

        print(f"\n   üìã –ù–ê–£–ß–ù–û –û–ë–û–°–ù–û–í–ê–ù–ù–´–ï –í–´–í–û–î–´:")
        if total_significant > 0:
            print(
                f"   ‚Ä¢ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {total_significant} –∑–Ω–∞—á–∏–º—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π (—Ç—Ä–µ–±—É–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è)"
            )
        else:
            print(f"   ‚Ä¢ –ó–Ω–∞—á–∏–º—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
            print(f"   ‚Ä¢ –ù–û —ç—Ç–æ –ù–ï –æ–∑–Ω–∞—á–∞–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∞ –ø—Ä–∏ –º–∞–ª–æ–π –≤—ã–±–æ—Ä–∫–µ")

        print(f"\n   üî¨ –ú–ï–¢–û–î–û–õ–û–ì–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´:")
        print(f"   ‚Ä¢ –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –æ—à–∏–±–æ–∫ I –∏ II —Ç–∏–ø–∞")
        print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä—ã —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã –ø—Ä–∏ p ‚â• {ALPHA_LEVEL}")
        print(f"   ‚Ä¢ –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–æ–ø—Ä–∞–≤–æ–∫ –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        print(f"   ‚Ä¢ –ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–µ–ø–ª–∏–∫–∞—Ü–∏–∏ –Ω–∞ –±–æ–ª—å—à–µ–π –≤—ã–±–æ—Ä–∫–µ")

    else:
        # –≠—Ç–æ—Ç –±–ª–æ–∫ –≤—ã–ø–æ–ª–Ω–∏—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
        if total_significant > 0 or total_large_effects > 5:
            print("   ‚úÖ –ù–ê–ô–î–ï–ù–´ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å—Ç—Ä–µ—Å—Å–∞")
            print("   üéØ –ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã:")

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            best_results = all_significant + all_large_effects
            unique_results = {r["measure_name"]: r for r in best_results}.values()
            sorted_results = sorted(
                unique_results, key=lambda x: abs(x["cohens_d"]), reverse=True
            )

            for result in sorted_results[:5]:
                level = "—Å–ª–æ–≤–∞" if "change_direction" in result else "—Ç—Ä–∞–π–ª—ã"
                print(f"      ‚Ä¢ {result['measure_name']} (—É—Ä–æ–≤–µ–Ω—å: {level})")
        else:
            print("   üìä –ü—Ä–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ —Ä–∞–∑–ª–∏—á–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
            print("   üéØ –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
            print("      ‚Ä¢ –ê–π—Ç—Ä–µ–∫–∏–Ω–≥ –Ω–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –¥–∞–Ω–Ω–æ–º—É –≤–∏–¥—É —Å—Ç—Ä–µ—Å—Å–∞")
            print("      ‚Ä¢ –ù–µ–æ–±—Ö–æ–¥–∏–º—ã –±–æ–ª–µ–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è")
            print("      ‚Ä¢ –ë—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ —Å—Ç—Ä–µ—Å—Å–æ–≤—ã–º —Å—Ç–∏–º—É–ª–∞–º")

    print(f"\nüîç –ù–ê–ò–ë–û–õ–ï–ï –ü–ï–†–°–ü–ï–ö–¢–ò–í–ù–´–ï –ú–ê–†–ö–ï–†–´ (–ø–æ —Ä–∞–∑–º–µ—Ä—É —ç—Ñ—Ñ–µ–∫—Ç–∞):")

    all_effects = sorted(
        word_comparison_results + trial_comparison_results,
        key=lambda x: abs(x["cohens_d"]),
        reverse=True,
    )

    for i, result in enumerate(all_effects[:8], 1):
        effect_description = (
            "–±–æ–ª—å—à–æ–π"
            if abs(result["cohens_d"]) >= 0.8
            else "—Å—Ä–µ–¥–Ω–∏–π"
            if abs(result["cohens_d"]) >= 0.5
            else "–º–∞–ª—ã–π"
        )
        level = "—Å–ª–æ–≤–∞" if "change_direction" in result else "—Ç—Ä–∞–π–ª—ã"
        print(
            f"   {i}. {result['measure_name']} ({level}): {effect_description} —ç—Ñ—Ñ–µ–∫—Ç (d = {result['cohens_d']:.3f})"
        )

    # –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò
    print(f"\nüí° –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —á–µ—Å—Ç–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    if min_sample_size < MIN_SAMPLE_SIZE_WARNING:
        print("   üö® –ù–ê –û–°–ù–û–í–ï –î–ê–ù–ù–û–ì–û –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:")
        print("   ‚Ä¢ –ù–ï–õ–¨–ó–Ø –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–π—Ç—Ä–µ–∫–∏–Ω–≥ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å—Ç—Ä–µ—Å—Å–∞")
        print("   ‚Ä¢ –ù–ï–õ–¨–ó–Ø —É—Ç–≤–µ—Ä–∂–¥–∞—Ç—å, —á—Ç–æ –º–µ—Ç–æ–¥ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
        print("   ‚Ä¢ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–æ—Å–∏—Ç –ü–ò–õ–û–¢–ù–´–ô —Ö–∞—Ä–∞–∫—Ç–µ—Ä")

        print(f"\n   üìã –ù–ï–û–ë–•–û–î–ò–ú–´–ï –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
        print(
            f"   1. –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –£–≤–µ–ª–∏—á–∏—Ç—å –≤—ã–±–æ—Ä–∫—É –¥–æ N ‚â• {MIN_SAMPLE_SIZE_WARNING}"
        )
        print(f"   2. –ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø–æ–ø—Ä–∞–≤–∫–∏ –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        print(f"   3. –ü—Ä–æ–≤–µ—Å—Ç–∏ power-–∞–Ω–∞–ª–∏–∑ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ N")
        print(f"   4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –≥—Ä—É–ø–ø—ã –∏ —Ä–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏—é")
        print(f"   5. –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π –≤—ã–±–æ—Ä–∫–µ")

        print(f"\n   üî¨ –ú–ï–¢–û–î–û–õ–û–ì–ò–ß–ï–°–ö–ò–ï –£–õ–£–ß–®–ï–ù–ò–Ø:")
        print(f"   ‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–¥–ø–æ—Å—ã–ª–æ–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤")
        print(f"   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–º–µ—à–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (mixed-effects)")
        print(f"   ‚Ä¢ –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–æ–∫")
        print(f"   ‚Ä¢ –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –ª–∏–Ω–∏–∏ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤")

        if total_significant == 0:
            print(f"\n   üìä –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –û–¢–°–£–¢–°–¢–í–ò–Ø –ó–ù–ê–ß–ò–ú–û–°–¢–ò:")
            print(f"   ‚Ä¢ –ù–ï –æ–∑–Ω–∞—á–∞–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∞")
            print(f"   ‚Ä¢ –ù–ï –æ–∑–Ω–∞—á–∞–µ—Ç –Ω–∞–ª–∏—á–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∞")
            print(f"   ‚Ä¢ –£–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—É—é –º–æ—â–Ω–æ—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
            print(f"   ‚Ä¢ –¢—Ä–µ–±—É–µ—Ç —Ä–µ–ø–ª–∏–∫–∞—Ü–∏–∏ —Å –±–æ–ª—å—à–µ–π –≤—ã–±–æ—Ä–∫–æ–π")

    else:
        # –≠—Ç–æ—Ç –±–ª–æ–∫ –≤—ã–ø–æ–ª–Ω–∏—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
        if total_significant > 0 or total_large_effects > 3:
            print("   ‚úÖ –ê–ô–¢–†–ï–ö–ò–ù–ì –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å—Ç—Ä–µ—Å—Å–∞")
            print("   üéØ –ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:")
            key_measures = (all_significant + all_large_effects)[:5]
            for result in key_measures:
                level = "—Å–ª–æ–≤–∞" if "change_direction" in result else "—Ç—Ä–∞–π–ª—ã"
                print(f"      ‚Ä¢ {result['measure_name']} (—É—Ä–æ–≤–µ–Ω—å: {level})")
            print("   üìä –ù–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:")
            print("      ‚Ä¢ –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
            print("      ‚Ä¢ –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –ª–∏–Ω–∏–∏")
            print("      ‚Ä¢ –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ñ–∏–∑–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏")
        else:
            print("   üìä –ê–ô–¢–†–ï–ö–ò–ù–ì –ù–ï –ø–æ–∫–∞–∑–∞–ª —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –≤–∏–¥–∞ —Å—Ç—Ä–µ—Å—Å–∞")
            print("   üéØ –í–æ–∑–º–æ–∂–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è:")
            print("      ‚Ä¢ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ —Å—Ç—Ä–µ—Å—Å–æ–≤—ã—Ö —Å—Ç–∏–º—É–ª–æ–≤")
            print("      ‚Ä¢ –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏")
            print("      ‚Ä¢ –§–æ–∫—É—Å –Ω–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏—è—Ö")

    print(f"\nüöÄ –ß–ï–°–¢–ù–û–ï –†–ï–ó–Æ–ú–ï –ü–ò–õ–û–¢–ù–û–ì–û –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:")
    print(
        f"   ‚Ä¢ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(word_comparison_results) + len(trial_comparison_results)} –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –∞–π—Ç—Ä–µ–∫–∏–Ω–≥–∞"
    )
    print(
        f"   ‚Ä¢ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {total_significant} –∏–∑ {len(word_comparison_results) + len(trial_comparison_results)}"
    )
    print(
        f"   ‚Ä¢ –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–π–ª–æ–≤: N = {min_sample_size} (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–∞–ª)"
    )
    print(f"   ‚Ä¢ –ü–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: {recovery_patterns} –∏–∑ {len(dynamics)}")

    if min_sample_size < MIN_SAMPLE_SIZE_WARNING:
        print(
            f"   ‚Ä¢ üö® –û–°–ù–û–í–ù–û–ô –í–´–í–û–î: –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ü–ò–õ–û–¢–ù–û–ï, –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å—Ç—Ä–µ—Å—Å–∞ –ù–ï–¢"
        )
        print(f"   ‚Ä¢ üìã –°–¢–ê–¢–£–°: –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Ç—Ä–∞–±–æ—Ç–∞–Ω–∞, —Ç—Ä–µ–±—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏")
        print(f"   ‚Ä¢ üéØ –ü–û–¢–ï–ù–¶–ò–ê–õ: –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –±–µ–∑ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏")
    else:
        potential = (
            "–≤—ã—Å–æ–∫–∏–π"
            if total_large_effects > 5
            else "—Å—Ä–µ–¥–Ω–∏–π"
            if total_large_effects > 2
            else "–Ω–∏–∑–∫–∏–π"
        )
        print(f"   ‚Ä¢ üéØ –ü–û–¢–ï–ù–¶–ò–ê–õ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å—Ç—Ä–µ—Å—Å–∞: {potential}")
        if total_significant > 0:
            print(f"   ‚Ä¢ ‚úÖ –ù–ê–ô–î–ï–ù–´ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ —Ä–∞–∑–ª–∏—á–∏—è")
        else:
            print(f"   ‚Ä¢ ‚ùå –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")


def print_research_hypotheses():
    """
    –ü–µ—á–∞—Ç–∞–µ—Ç —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    """
    print("üî¨ –ù–ê–£–ß–ù–´–ï –ì–ò–ü–û–¢–ï–ó–´ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø")
    print("=" * 80)

    print("\nüìã –§–û–†–ú–£–õ–ò–†–û–í–ö–ê –ì–ò–ü–û–¢–ï–ó:")

    print("\nüî∏ –ù–£–õ–ï–í–ê–Ø –ì–ò–ü–û–¢–ï–ó–ê (H0):")
    print("   –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –∞–π—Ç—Ä–µ–∫–∏–Ω–≥–∞")
    print(
        "   –º–µ–∂–¥—É —É—Å–ª–æ–≤–∏—è–º–∏ –ë–ï–ó –°–¢–†–ï–°–°–ê (—Ç—Ä–∞–π–ª—ã 1-{}) –∏ –°–û –°–¢–†–ï–°–°–û–ú (—Ç—Ä–∞–π–ª—ã {}-{})".format(
            STRESS_THRESHOLD, STRESS_THRESHOLD + 1, MAX_TRIAL_NUMBER
        )
    )
    print("   H0: Œº‚ÇÅ = Œº‚ÇÇ (—Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ä–∞–≤–Ω—ã)")

    print("\nüîπ –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–ê–Ø –ì–ò–ü–û–¢–ï–ó–ê (H1):")
    print("   –°—É—â–µ—Å—Ç–≤—É—é—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –∞–π—Ç—Ä–µ–∫–∏–Ω–≥–∞")
    print("   –º–µ–∂–¥—É —É—Å–ª–æ–≤–∏—è–º–∏ –ë–ï–ó –°–¢–†–ï–°–°–ê –∏ –°–û –°–¢–†–ï–°–°–û–ú")
    print("   H1: Œº‚ÇÅ ‚â† Œº‚ÇÇ (—Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è)")

    print("\n‚öñÔ∏è –ö–†–ò–¢–ï–†–ò–ò –ü–†–ò–ù–Ø–¢–ò–Ø –†–ï–®–ï–ù–ò–Ø:")
    print(f"   ‚Ä¢ –£—Ä–æ–≤–µ–Ω—å –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏: Œ± = {ALPHA_LEVEL}")
    print(f"   ‚Ä¢ –ï—Å–ª–∏ p-value < {ALPHA_LEVEL} ‚Üí –û–¢–ö–õ–û–ù–Ø–ï–ú H0, –ü–†–ò–ù–ò–ú–ê–ï–ú H1")
    print(f"   ‚Ä¢ –ï—Å–ª–∏ p-value ‚â• {ALPHA_LEVEL} ‚Üí –ù–ï –û–¢–ö–õ–û–ù–Ø–ï–ú H0")

    print("\nüß™ –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ï –ú–ï–¢–û–î–´:")
    print("   ‚Ä¢ –ö—Ä–∏—Ç–µ—Ä–∏–π –ú–∞–Ω–Ω–∞-–£–∏—Ç–Ω–∏ (–Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π)")
    print("   ‚Ä¢ t-–∫—Ä–∏—Ç–µ—Ä–∏–π –°—Ç—å—é–¥–µ–Ω—Ç–∞ (–ø—Ä–∏ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏)")
    print("   ‚Ä¢ –†–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞: Cohen's d")

    print("\nüìä –û–ñ–ò–î–ê–ï–ú–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print("   –ï—Å–ª–∏ H1 –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è ‚Üí –∞–π—Ç—Ä–µ–∫–∏–Ω–≥ –º–æ–∂–µ—Ç –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–µ—Å—Å")
    print("   –ï—Å–ª–∏ H0 –Ω–µ –æ—Ç–∫–ª–æ–Ω—è–µ—Ç—Å—è ‚Üí –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∞–π—Ç—Ä–µ–∫–∏–Ω–≥–∞
    """
    # –ü–∞—Ä—Å–∏–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    args = parse_arguments()

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    global WORD_DATA_FILE, TRIAL_DATA_FILE, RESULTS_DIR, SHOW_PLOTS
    WORD_DATA_FILE = args.word_file
    TRIAL_DATA_FILE = args.trial_file
    RESULTS_DIR = args.results_dir

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–ª–∞–≥ –ø–æ–∫–∞–∑–∞ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    if args.show_plots:
        SHOW_PLOTS = True
    else:
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏
        SHOW_PLOTS = False

    print("üöÄ –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –ê–ô–¢–†–ï–ö–ò–ù–ì–ê")
    print("üéØ –¶–µ–ª—å: –î–æ–∫–∞–∑–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å—Ç—Ä–µ—Å—Å–∞ —á–µ—Ä–µ–∑ –¥–≤–∏–∂–µ–Ω–∏—è –≥–ª–∞–∑")
    print("üìä –ê–Ω–∞–ª–∏–∑ –Ω–∞ –¥–≤—É—Ö —É—Ä–æ–≤–Ω—è—Ö: –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ + —Ü–µ–ª—ã–µ —Ç—Ä–∞–π–ª—ã")
    print(f"üìÅ –î–∞–Ω–Ω—ã–µ —Å–ª–æ–≤: {WORD_DATA_FILE}")
    print(f"üìÅ –î–∞–Ω–Ω—ã–µ —Ç—Ä–∞–π–ª–æ–≤: {TRIAL_DATA_FILE}")
    print(f"üìä –ü–∞–ø–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {RESULTS_DIR}")
    print(f"üì∫ –ü–æ–∫–∞–∑ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {'–î–∞' if SHOW_PLOTS else '–ù–µ—Ç (—Ç–æ–ª—å–∫–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ)'}")
    if not SHOW_STATISTICAL_WARNINGS:
        print("üîá –†–µ–∂–∏–º: Warnings –ø–æ–¥–∞–≤–ª–µ–Ω—ã")
    print("=" * 80)

    # –§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –Ω–∞—É—á–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑
    print_research_hypotheses()

    try:
        # 0. –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        ensure_results_directory()

        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        word_data, trial_data, word_measure_names = load_comprehensive_data()

        # 2. –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤
        word_comparison_results = analyze_word_level_differences(
            word_data, word_measure_names
        )

        # 3. –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–π–ª–æ–≤
        trial_comparison_results = analyze_trial_level_differences(trial_data)

        # 4. –ê–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–π–ª–æ–≤
        dynamics, trial_stats = analyze_trial_dynamics(trial_data)

        # 5. –ê–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤
        word_trial_stats, word_dynamics_results = analyze_word_dynamics(
            word_data, word_measure_names
        )

        # 6. –°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤
        # 6a. –ì—Ä–∞—Ñ–∏–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —É—Å–ª–æ–≤–∏–π –¥–ª—è —Å–ª–æ–≤
        create_enhanced_word_visualizations(
            word_data, word_comparison_results, word_measure_names
        )

        # 6b. –ì—Ä–∞—Ñ–∏–∫–∏ –¥–∏–Ω–∞–º–∏–∫–∏ –¥–ª—è –æ–±–æ–∏—Ö —É—Ä–æ–≤–Ω–µ–π
        create_dynamics_visualizations(
            trial_data,
            trial_stats,
            word_data,
            word_trial_stats,
            word_dynamics_results,
            word_measure_names,
        )

        # 6c. –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–π–ª–æ–≤
        create_comprehensive_visualizations(
            trial_data, trial_comparison_results, dynamics
        )

        # 7. –§–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∏–ø–æ—Ç–µ–∑
        total_significant, total_tests = test_formal_hypotheses(
            word_comparison_results, trial_comparison_results
        )

        # 8. –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
        generate_comprehensive_report(
            word_comparison_results, trial_comparison_results, dynamics
        )

        print(f"\nüéâ –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
        print(f"üìä –°–æ–∑–¥–∞–Ω–æ 6 –Ω–∞–±–æ—Ä–æ–≤ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ –ø–∞–ø–∫–µ '{RESULTS_DIR}/':")
        print(
            f"   ‚Ä¢ {RESULTS_DIR}/word_level_stress_analysis.png - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —É—Å–ª–æ–≤–∏–π (—Å–ª–æ–≤–∞)"
        )
        print(f"   ‚Ä¢ {RESULTS_DIR}/trial_level_dynamics.png - –¥–∏–Ω–∞–º–∏–∫–∞ –ø–æ —Ç—Ä–∞–π–ª–∞–º")
        print(f"   ‚Ä¢ {RESULTS_DIR}/word_level_dynamics.png - –¥–∏–Ω–∞–º–∏–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º")
        print(
            f"   ‚Ä¢ {RESULTS_DIR}/comprehensive_stress_comparison.png - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —É—Å–ª–æ–≤–∏–π"
        )
        print(
            f"   ‚Ä¢ {RESULTS_DIR}/comprehensive_trial_dynamics.png - –¥–∏–Ω–∞–º–∏–∫–∞ –ø–æ —Ç—Ä–∞–π–ª–∞–º"
        )
        print(f"   ‚Ä¢ {RESULTS_DIR}/key_stress_markers.png - –∫–ª—é—á–µ–≤—ã–µ –º–∞—Ä–∫–µ—Ä—ã —Å—Ç—Ä–µ—Å—Å–∞")

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–æ—Ç–µ–∑
        print(f"\nüî¨ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –ì–ò–ü–û–¢–ï–ó:")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤: {total_tests}")
        print(f"   ‚Ä¢ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {total_significant}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —á–µ—Å—Ç–Ω—ã—Ö –≤—ã–≤–æ–¥–æ–≤
        trial_sample_size = TOTAL_TRIALS  # –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–π–ª–æ–≤
        validate_sample_size(trial_sample_size, "–∏—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã")

        if total_significant > 0:
            print(f"   üî∏ H0 –ß–ê–°–¢–ò–ß–ù–û –û–¢–ö–õ–û–ù–Ø–ï–¢–°–Ø ‚Üí H1 —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è")
            if trial_sample_size < MIN_SAMPLE_SIZE_WARNING:
                print(f"   ‚ö†Ô∏è –û–î–ù–ê–ö–û: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–∞–ª–∞—è –≤—ã–±–æ—Ä–∫–∞ (N={trial_sample_size})")
                print(
                    f"   üö® –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç—Ä–µ–±—É—é—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –Ω–∞ –±–æ–ª—å—à–µ–π –≤—ã–±–æ—Ä–∫–µ"
                )
                print(
                    f"   üìã –°–¢–ê–¢–£–°: –ü–ò–õ–û–¢–ù–û–ï –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –ù–ï –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å—Ç—Ä–µ—Å—Å–∞"
                )
            else:
                print(
                    f"   ‚úÖ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï: –ù–∞–π–¥–µ–Ω—ã –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ä–∞–∑–ª–∏—á–∏–π –≤ –∞–π—Ç—Ä–µ–∫–∏–Ω–≥–µ –ø—Ä–∏ —Å—Ç—Ä–µ—Å—Å–µ"
                )
        else:
            print(f"   üî∏ H0 –ù–ï –û–¢–ö–õ–û–ù–Ø–ï–¢–°–Ø –Ω–∞ —É—Ä–æ–≤–Ω–µ p < {ALPHA_LEVEL}")

            if trial_sample_size < MIN_SAMPLE_SIZE_WARNING:
                print(
                    f"   üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï: –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ N={trial_sample_size} –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–µ–Ω"
                )
                print(f"   üìã –ß–ï–°–¢–ù–û–ï –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï:")
                print(
                    f"   ‚Ä¢ –ù–ï–í–û–ó–ú–û–ñ–ù–û —É—Ç–≤–µ—Ä–∂–¥–∞—Ç—å, —á—Ç–æ –∞–π—Ç—Ä–µ–∫–∏–Ω–≥ –ù–ï –º–æ–∂–µ—Ç –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–µ—Å—Å"
                )
                print(
                    f"   ‚Ä¢ –ù–ï–í–û–ó–ú–û–ñ–ù–û —É—Ç–≤–µ—Ä–∂–¥–∞—Ç—å, —á—Ç–æ –∞–π—Ç—Ä–µ–∫–∏–Ω–≥ –ú–û–ñ–ï–¢ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–µ—Å—Å"
                )
                print(f"   ‚Ä¢ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —è–≤–ª—è–µ—Ç—Å—è –ü–ò–õ–û–¢–ù–´–ú")
                print(f"   ‚Ä¢ –û—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –æ—Ç—Ä–∞–±–æ—Ç–∫–∞ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏")
                print(
                    f"   ‚Ä¢ –¢—Ä–µ–±—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏ –¥–æ N ‚â• {MIN_SAMPLE_SIZE_WARNING}"
                )
            else:
                large_effects = len(
                    [
                        r
                        for r in word_comparison_results + trial_comparison_results
                        if abs(r["cohens_d"]) >= EFFECT_SIZE_LARGE
                    ]
                )
                if large_effects > 5:
                    print(
                        f"   üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {large_effects} –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —Å –±–æ–ª—å—à–∏–º —Ä–∞–∑–º–µ—Ä–æ–º —ç—Ñ—Ñ–µ–∫—Ç–∞"
                    )
                    print(
                        f"   üî¨ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï: –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –µ—Å—Ç—å, –Ω–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –Ω–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω"
                    )
                else:
                    print(
                        f"   üìä –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï: –ü—Ä–∏ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ —Ä–∞–∑–ª–∏—á–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ"
                    )

    except Exception as e:
        print(f"‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
